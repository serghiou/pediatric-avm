---
title: 'Predictive analytics: `r params$TITLE`'
author: "Stylianos Serghiou"
date: '`r format(Sys.time(), "%d/%m/%Y")`'
params:
  PROPORTION_OF_DATA:
    label: "Proportion of data to use"
    value: 1
    input: slider
    min: 0
    max: 1
    step: 0.05
    sep: ""
  CONFIG: 
    label: "Config file name"
    value: "config_is-poor-outcome.R"
    input: select
    choices: [
      "config_is-poor-outcome.R",
      "config_is-poor-outcome_haemTRUE.R",
      "config_is-poor-outcome_haemFALSE.R",
      "config_has-complication-major.R",
      "config_has-complication-minor.R",
      "config_is-obliterated.R"
    ]
  TITLE:
    label: "Outcome title"
    value: "Poor outcomes"
    input: select
    choices: [
      "Poor outcome",
      "Poor outcome - Hemorrhage",
      "Poor outcome - No Hemorrhage",
      "Complications - Major",
      "Complications - Minor",
      "Obliterated"
    ]
output:
  rmdformats::readthedown:
    highlight: kate
    df_print: kable    # obviates %>% kable; does not replace styling though
    code_folding: hide # [hide, show] (comment out to not give option)
  pdf_document:
    highlight: tango
    df_print: kable
    latex_engine: pdflatex
    keep_tex: yes
  tufte::tufte_handout: default
  tufte::tufte_html: 
    toc: TRUE
  prettydoc::html_pretty:
    # no cold_folding available
    theme: hpstr      # or: architect; https://github.com/yixuan/prettydoc
    highlight: github # or: vignette
    toc: TRUE         # no toc_float available
    df_print: kable   # obviates %>% kable; does not replace styling though
header-includes:
- \DeclareUnicodeCharacter{3B8}{~}
- \DeclareUnicodeCharacter{3B1}{~}
- \DeclareUnicodeCharacter{3B2}{~}
- \DeclareUnicodeCharacter{223C}{~}
- \DeclareUnicodeCharacter{2264}{~}
- \DeclareUnicodeCharacter{2265}{~}
- \DeclareUnicodeCharacter{2581}{~}
- \DeclareUnicodeCharacter{2582}{~}
- \DeclareUnicodeCharacter{2583}{~}
- \DeclareUnicodeCharacter{2585}{~}
- \DeclareUnicodeCharacter{2586}{~}
- \DeclareUnicodeCharacter{2587}{~}
- \DeclareUnicodeCharacter{FB00}{~}
- \usepackage{graphicx}
editor_options: 
  chunk_output_type: inline
---

<style>
p {

text-align: justify;
text-justify: interword;
padding: 0 0 0.5em 0

}
</style>

```{r knitr, echo=FALSE}
# Load packages
library(knitr)
library(rmdformats)
library(kableExtra)
library(ggplot2)
library(magrittr)


######### knitr

# Define chunk options
opts_chunk$set(
  echo = T,
  cache = F, # if TRUE, no need to rerun chunks
  # cache.lazy = TRUE,  # use with big objects (>1 GB)
  cache.comments = F, # do not rebuild if comments change
  tidy = F, # can play with this
  warning = F,
  message = F,
  comment = NA,
  fig.align = "center",
  fig.width = 7,
  fig.path = "../../outputs/predictive-analytics/figs/", # export all figures
  linewidth = 91,
  width = 75
)

# Initiatialize hook
hook_output <- knit_hooks$get("output")

# Hook to wrap output text when it exceeds 'n' using linewidth
knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$linewidth)) {
    x <- knitr:::split_lines(x)

    # wrap lines wider than 'n'
    if (any(nchar(x) > n)) {
      x <- strwrap(x, width = n)
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})

# Times a chunk and prints the time it took to run it under the chunk
# To time a chunk, include in the chunk options: {r my_chunk, timeit=TRUE}
knitr::knit_hooks$set(timeit = local({
  now <- NULL
  function(before, options) {
    if (before) {
      now <<- Sys.time()
    } else {
      res <- difftime(Sys.time(), now)
      now <<- NULL
      # use options$label if you want the chunk label as well
      paste("Time for this code chunk:", as.character(res))
    }
  }
}))

# For more knitr options visit: https://yihui.name/knitr/options/
# and his github page: https://github.com/yihui/knitr-examples


######### kableExtra

options(
  knitr.kable.NA = "", # replace NAs in tables with blank
  digits = 5 # round digits (doesn't work without this here!)
)

## Example use
# data.frame(x = c(1,2,3), y = c(4,5,6), z = c(7,8,9)) %>%
#   kable(booktabs = T) %>% kable_styling()

# Function to simplify table styling
sable <- function(tab, escape = T, full_width = F, drop = F, font_size = 12) {
  if (drop) {
    tab %>%
      kable(escape = escape, booktabs = T) %>%
      collapse_rows(valign = "top") %>%
      kable_styling(
        "striped",
        position = "center",
        full_width = full_width,
        font_size = font_size
      )
  } else {
    tab %>%
      kable(escape = escape, booktabs = T) %>%
      kable_styling(
        "striped",
        position = "center",
        full_width = full_width,
        font_size = font_size
      )
  }
}

## Guidelines
# No longer need to define options(knitr.table.format = "html"). It is now automatically done as soon as you load kableExtra
# No need to run kable() every time - done automatically as soon as you load kableExtra
# Loading kableExtra nullifies any styling applied by df_table: kable in the preamble - if you are content with standard formatting, DO NOT load kableExtra


#########  ggplot2

# Set up preferred theme in ggplot2
my_theme <-
  # this and theme_minimal() are my favorite
  theme_light() +
  theme(
    axis.ticks = element_blank(),
    axis.title = element_text(face = "bold"),
    axis.title.x = element_text(margin = margin(t = 15)),
    axis.title.y = element_text(margin = margin(r = 7)),
    legend.key = element_rect(colour = NA, fill = NA), # Avoid borders
    panel.border = element_blank(),
    text = element_text(color = "grey20"),
    title = element_text(face = "bold")
  )

# Make the above theme the default theme
original_theme <- theme_set(my_theme)

# Use ggplot2::ggsave to save plots after plotting - this reduces size dramatically


######### Tabbed sections

# You can organize content using tabs by applying the .tabset class attribute to headers within a document. This will cause all sub-headers of the header with the .tabset attribute to appear within tabs rather than as standalone sections. For example:

## Quarterly Results {.tabset}

### By Product


######### Update package

# To update the package use:
# devtools::install_github(serghiou/serghioutemplates)
```


# Thoughts

## Questions

- What deficit are we talking about? Is it covered by the mRS?
- Should the pre-treatment mRS be in the model? Already covered by the others?


## Backlog

- Honest estimation
- G estimation (using only significant ones to get accurate ORs)
- Make sure that the number of outcomes (26) is valid.
- Bootstrap the whole process - how many models include each of the features?
- The SuSiE method: https://www.biorxiv.org/content/10.1101/501114v1
- Choose best model with forward/backward exclusion
- Feature selection with honest estimation + p-value-based + bootstrap
- Feature selection with honest estimation + LASSO-based + bootstrap
- Implement PCA to create independent variables and not drop variables
- Bayesian fitting
- Change to use tidybayes and tidymodels
- Use neural networks to fit
- Consider a time-to-event analysis


------------------------------------------------------------------------

# Setup

## Imports

```{r setup}
# Load packages
library(glmnet)
library(magrittr)
library(patchwork)
library(selectiveInference)
library(tidyverse)

# Data
# https://office365stanford-my.sharepoint.com/:x:/r/personal/simonlev_stanford_edu/_layouts/15/Doc.aspx?sourcedoc=%7BCE7DAF19-8F1A-43B0-866D-3D0F36336EDC%7D&file=Peds%20AVM%20Database%20July%2010%202024%20Onwards.xlsx&fromShare=true&action=default&mobileredirect=true

# Source code
```


## Parameters

```{r}
# Source outcome-specific configs
source(params$CONFIG)

# Destination locations
DST_DIRNAME <- paste0("../../outputs/")

# Source locations
SRC_DIRNAME <- "../../data/3_tidy/patients"
SRC_BASENAME <- "patients-daily_2024-07-21.csv"

# Should the Dockerfile be automatically updated?
UPDATE_DOCKERFILE <- FALSE

# Set the seed
set.seed(1891)
```


## Variables

```{r}
EXPOSURES_CONTINUOUS <- c(
  "Age at 1st treatment (years)" = "age_at_first_treatment_yrs",
  "mRS (presentation)" = "modified_rankin_score_presentation",
  "mRS (pre-treatment)" = "modified_rankin_score_pretreatment",
  "mRS (1-week post-op)" = "modified_rankin_score_postop_within_1_week",
  "mRS (final)" = "modified_rankin_score_final",
  "Nidus size (cm)" = "max_size_cm",
  "Spetzler-Martin grade" = "spetzler_martin_grade",
  "Lawton-Young grade" = "lawton_young_grade",
  "Size score" = "size_score"
)

EXPOSURES_BINARY <- c(
  "Sex (Male)" = "is_male",
  "First tx after Jun 2009" = "is_first_tx_after_2009",
  "Involves eloquent location" = "is_eloquent_location",
  "Has deep venous drainage" = "has_deep_venous_drainage",
  "Diffuse nidus" = "is_diffuse_nidus",
  "Hemorrhage at presentation" = "has_hemorrhage",
  "Seizures at presentation" = "has_seizures",
  "Deficit at presentation" = "has_deficit",
  "Paresis at presentation" = "has_paresis",
  "Presence of aneurysms" = "has_associated_aneurysm",
  "Spetzler-Martin grade < 4" = "is_spetzler_martin_grade_less_than_4",
  "Had surgery" = "is_surgery"
)

EXPOSURES_CATEGORICAL <- c(
  "Location" = "location",
  "Venous drainage" = "venous_drainage",
  "Modality of treatment" = "procedure_combinations"
)

OUTCOMES <- c(
  "Poor outcome (mRS >= 3)" = "is_poor_outcome",
  "Obliteration" = "is_obliterated",
  "Complications - minor" = "has_complication_minor",
  "Complications - major" = "has_complication_major",
  "mRS change (final - pre-treatment)" =
    "modified_rankin_score_final_minus_pretx",
  "mRS change (final - presentation)" =
    "modified_rankin_score_final_minus_presentation",
  "mRS change direction (Final - Pre-tx)" = "change_in_mrs_tx_vs_final"
)
```


## Outputs

Create lists.

```{r}
plots <- list()
tables <- list()
models <- list()
```


## Functions

### R utils

```{r}
```


### Data analysis utils

Parse inconsistent dates.

```{r}
# Function to parse dates using the appropriate format
# (needed b/c some are "%m/%d/%Y" and some "%m/%d/%y")
parse_inconsistent_dates <- function(date_col) {
  # Try %m/%d/%y format first
  parsed_dates <- as.Date(date_col, format = "%m/%d/%Y")
  
  # Identify and correct dates before 1950
  parsed_dates <- ifelse(
    !is.na(parsed_dates) & parsed_dates < as.Date("1950-01-01"),
    as.Date(date_col, format = "%m/%d/%y"),
    parsed_dates
  )
  
  # Return corrected dates
  as.Date(parsed_dates, origin = "1970-01-01")
}
```


Descriptive statistics - continuous variables.

```{r}
compute_descriptive_continuous <- function(
    df = df_uni,
    cols = c(EXPOSURES_CONTINUOUS),
    outcome_col = CHOSEN_OUTCOME,
    outcome_levels = CHOSEN_OUTCOME_LEVELS,
    outcome_labels = CHOSEN_OUTCOME_LABELS,
    use_interaction = TRUE,
    interaction_col = "has_hemorrhage",
    interaction_col_name = "Hemorrhage at presentation",
    interaction_col_value = TRUE,
    prefix = "Haem") {
  
  # Apply desired formatting to numbers
  format_numbers <- function(x) {
    sprintf("%.1f", x)
  }

  # Compute new dataset
  df <-
    df %>%
    select(cols, outcome_col, interaction_col) %>%
    drop_na(outcome_col, interaction_col)

  # Apply interaction
  if (use_interaction) {
    df <-
      df %>%
      filter(!!sym(interaction_col) == interaction_col_value)
  }

  # Compute descriptive statistics
  desc_stats <-
    df %>%
    pivot_longer(where(is.numeric), names_to = "keys", values_to = "values") %>%
    group_by(!!sym(outcome_col), keys) %>%
    summarize(
      num_total = n(),
      num_missing = sum(is.na(values)),
      mean = mean(values, na.rm = TRUE),
      sd = sd(values, na.rm = TRUE),
      min = quantile(values, 0, na.rm = TRUE),
      max = quantile(values, 1, na.rm = TRUE),
      q_50 = quantile(values, 0.50, na.rm = TRUE),
      q_25 = quantile(values, 0.25, na.rm = TRUE),
      q_75 = quantile(values, 0.75, na.rm = TRUE)
    ) %>%
    ungroup() %>%
    mutate(
      stats = glue::glue("{format_numbers(mean)} ({format_numbers(sd)})"),
      !!sym(outcome_col) := factor(
        !!sym(outcome_col),
        levels = outcome_levels,
        labels = outcome_labels
      )
    ) %>%
    pivot_wider(
      id_cols = keys,
      names_from = !!sym(outcome_col),
      values_from = stats
    ) %>%
    relocate(outcome_labels[2], .after = outcome_labels[1])

  # Compute p-values
  pvals <-
    df %>%
    pivot_longer(where(is.numeric), names_to = "keys", values_to = "values") %>%
    group_by(keys) %>%
    summarize(
      pvalue = wilcox.test(values ~ !!sym(outcome_col))$p.value
    )

  # Synthesize
  out <-
    desc_stats %>%
    left_join(pvals)

  # Add sample size to column names
  num_with_outcome <- sum(df %>% pull(outcome_col))
  num_no_outcome <- sum(!df %>% pull(outcome_col))

  new_col_names <- c(
    glue::glue("{prefix} - {outcome_labels[2]} (n={num_with_outcome})"),
    glue::glue("{prefix} - {outcome_labels[1]} (n={num_no_outcome})"),
    glue::glue("{prefix} - P-value")
  )

  out <-
    out %>%
    rename_with(
      .cols = !!sym(outcome_labels[1]):pvalue, ~new_col_names
    ) %>%
    mutate(
      keys = ifelse(keys == interaction_col, interaction_col_name, keys)
    )

  return(out)
}
```


Descriptive statistics - binary variables.

```{r}
compute_descriptive_binary <- function(
    df = df_uni,
    cols = c(EXPOSURES_BINARY),
    outcome_col = CHOSEN_OUTCOME,
    outcome_levels = CHOSEN_OUTCOME_LEVELS,
    outcome_labels = CHOSEN_OUTCOME_LABELS,
    use_interaction = TRUE,
    interaction_col = "has_hemorrhage",
    interaction_col_name = "Hemorrhage at presentation",
    interaction_col_value = TRUE,
    prefix = "Haem") {
  
  # Define arguments
  cols <- c(cols[!cols == interaction_col])

  # Apply desired formatting to numbers
  format_numbers <- function(x, decimals = 0) {
    decimals <- paste0("%.", decimals, "f")
    sprintf(decimals, x)
  }

  # Compute new dataset
  df <-
    df %>%
    select(all_of(cols), outcome_col, interaction_col) %>%
    drop_na(outcome_col, interaction_col)

  # Apply interaction
  if (use_interaction) {
    df <-
      df %>%
      filter(!!sym(interaction_col) == interaction_col_value)
  }

  # Compute descriptive statistics
  desc_stats <-
    df %>%
    pivot_longer(
      cols = -c(outcome_col),
      names_to = "keys",
      values_to = "values"
    ) %>%
    group_by(!!sym(outcome_col), keys) %>%
    summarize(
      num_total = n(),
      num_missing = sum(is.na(values)),
      num_with = sum(values, na.rm = TRUE),
      num_without = sum(!values, na.rm = TRUE),
      pct_with = mean(values, na.rm = TRUE)
    ) %>%
    ungroup() %>%
    mutate(
      stats = glue::glue("{num_with} ({format_numbers(pct_with*100)}%)"),
      !!sym(outcome_col) := factor(
        !!sym(outcome_col),
        levels = outcome_levels,
        labels = outcome_labels
      )
    ) %>%
    pivot_wider(
      id_cols = keys,
      names_from = !!sym(outcome_col),
      values_from = stats
    ) %>%
    relocate(outcome_labels[1], .after = outcome_labels[2])

  # Compute p-values
  pvals <-
    df %>%
    pivot_longer(
      cols = -c(outcome_col),
      names_to = "keys",
      values_to = "values"
    ) %>%
    group_by(keys) %>%
    summarize(
      pvalue = kruskal.test(values ~ !!sym(outcome_col))$p.value
    )

  # Synthesize
  out <-
    desc_stats %>%
    left_join(pvals)

  # Prettify
  num_with_outcome <- sum(df %>% pull(outcome_col))
  num_no_outcome <- sum(!df %>% pull(outcome_col))

  new_col_names <- c(
    glue::glue("{prefix} - {outcome_labels[2]} (n={num_with_outcome})"),
    glue::glue("{prefix} - {outcome_labels[1]} (n={num_no_outcome})"),
    glue::glue("{prefix} - P-value")
  )

  out <-
    out %>%
    rename_with(
      .cols = !!sym(outcome_labels[2]):pvalue, ~new_col_names
    ) %>%
    mutate(
      keys = ifelse(keys == interaction_col, interaction_col_name, keys)
    )

  return(out)
}
```


------------------------------------------------------------------------

# Read

```{r}
# File path
filepath <- file.path(SRC_DIRNAME, SRC_BASENAME)

# Read
patients_ <-
  read_csv(
    filepath,
    col_types = cols(
      .default = col_guess(), # Default behavior for other columns
      date_first_treatment = col_character()
    )
  ) %>%
  dplyr::sample_frac(params$PROPORTION_OF_DATA)

# If analyzing a subset of the data, restrict the dataset
if (params$TITLE == "Poor outcome - Hemorrhage") {
  patients_ <- patients_ %>% filter(has_hemorrhage)
}
if (params$TITLE == "Poor outcome - No Hemorrhage") {
  patients_ <- patients_ %>% filter(!has_hemorrhage)
}
if (params$TITLE == "Poor outcome - High grade") {
  patients_ <- patients_ %>% filter(spetzler_martin_grade >= 4)
}
if (params$TITLE == "Poor outcome - Low grade") {
  patients_ <- patients_ %>% filter(spetzler_martin_grade < 4)
}
```


------------------------------------------------------------------------

# Conform

## Setup

Remove all empty rows.

```{r}
patients_ <-
  patients_ %>%
  filter(!is.na(patient_id)) %>%
  arrange(patient_id)
```


Create two dataframes - one for univariable and one for multivariable analysis. This is because variables for multivariable analysis will be recoded to reduce the number of levels to avoid overfitting the model.

```{r}
df_uni <-
  patients_ %>%
  filter(is_eligible) %>% 
  filter(!is.na(spetzler_martin_grade))

df_multi <-
  patients_ %>%
  filter(is_eligible) %>% 
  filter(!is.na(spetzler_martin_grade))
```


## Recode columns

For `venous_drainage` if "Both", mark as "Deep."

```{r}
df_multi <-
  df_multi %>%
  mutate(
    venous_drainage = ifelse(venous_drainage == "Both", "Deep", venous_drainage)
  )
```


For `procedure_combinations`, change > 1 to multimodal to reduce levels.

```{r}
df_multi <-
  df_multi %>%
  mutate(procedure_combinations = case_when(
    nchar(procedure_combinations) > 1 ~ "Multimodal",
    .default = procedure_combinations
  ))
```


For `procedure_combinations`, indicate if surgery-based or not.

```{r}
df_multi <-
  df_multi %>%
  mutate(
    is_surgery = str_detect(procedure_combinations, "S")
  ) %>%
  relocate(is_surgery, .after = procedure_combinations)

df_uni <-
  df_uni %>%
  mutate(
    is_surgery = str_detect(procedure_combinations, "S")
  ) %>%
  relocate(is_surgery, .after = procedure_combinations)
```


For `location`, reduce choices (use "Other").

```{r}
df_multi <-
  df_multi %>%
  mutate(
    location = ifelse(location == "Corpus Callosum", "Other", location),
    location = ifelse(location == "Cerebellum", "Other", location),
    # location = ifelse(location == "Deep", "Other", location),
    location = factor(location),
    location = relevel(location, ref = "Other")
  )
```


For `spetzler_martin_grade`, create a binary variant of 1-3 vs. 4-5.

```{r}
# For multivariable analysis
df_multi <-
  df_multi %>%
  mutate(
    # Divides into low grade vs. high grade
    is_spetzler_martin_grade_less_than_4 = spetzler_martin_grade < 4
  ) %>%
  relocate(is_spetzler_martin_grade_less_than_4, .after = spetzler_martin_grade)

# For univariable analysis
df_uni <-
  df_uni %>%
  mutate(
    is_spetzler_martin_grade_less_than_4 = spetzler_martin_grade < 4
  ) %>%
  relocate(is_spetzler_martin_grade_less_than_4, .after = spetzler_martin_grade)
```


For `date_first_treatment`,transform into dates and create > 2009 vs. not.

```{r}
# Parse into Date format and create dichotomous variable
df_uni <-
  df_uni %>%
  mutate(across(starts_with("date"), ~ parse_inconsistent_dates(.x))) %>% 
  mutate(is_first_tx_after_2009 = date_first_treatment >= as.Date("2009-06-30"))

# Fix dates and parse into Date format
df_multi <-
  df_multi %>%
  mutate(across(starts_with("date"), ~ parse_inconsistent_dates(.x))) %>% 
  mutate(is_first_tx_after_2009 = date_first_treatment >= as.Date("2009-06-30"))

# Check for missingness
num_missing <-
  df_uni %>% 
  filter(is.na(date_first_treatment)) %>% 
  pull(patient_id) %>% 
  length()

ids_missing <- 
  df_uni %>%
  filter(is.na(date_first_treatment)) %>% 
  pull(patient_id)

# Check for non-sensical data
min(df_uni$date_first_treatment, na.rm = T) > as.Date("1980-01-01")
max(df_uni$date_first_treatment, na.rm = T) < as.Date("2025-01-01")
```


## Missingness

```{r}
# Get cols
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  "is_first_tx_after_2009",
  EXPOSURES_CATEGORICAL,
  OUTCOMES
))

# Count missingness
df_multi %>%
  select(cols) %>%
  summarise_all(~ sum(is.na(.))) %>%
  pivot_longer(everything(), values_to = "num_missing") %>%
  arrange(desc(num_missing)) %>%
  filter(num_missing > 0) %>%
  sable()
```


Which eligible patients are missing each variable?

```{r}
df_multi %>%
  filter(is_eligible) %>%
  select(mrn, cols) %>%
  mutate(across(-mrn, is.na)) %>%
  pivot_longer(
    cols = -mrn,
    names_to = "name",
    values_to = "is_missing"
  ) %>%
  filter(is_missing) %>%
  group_by(name) %>%
  summarize(mrn = str_c(mrn, collapse = ", ")) %>%
  sable()
```


------------------------------------------------------------------------

# Visualizations

## Overall

Distribution of values of the exposures across levels of the outcome.

```{r, fig.width=12, fig.height=8}
# Define the predictors of interest
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  CHOSEN_OUTCOME
)

# Reshape the data
df_long <-
  df_uni %>%
  select(all_of(cols)) %>%
  pivot_longer(-CHOSEN_OUTCOME, names_to = "predictor", values_to = "value")

# Plot the box plots
df_long %>%
  ggplot(aes_string(
    x = CHOSEN_OUTCOME,
    y = "value",
    fill = CHOSEN_OUTCOME,
    color = CHOSEN_OUTCOME
  )) +
  geom_boxplot(alpha = 0.5) +
  facet_wrap(~predictor, scales = "free") +
  labs(x = "Outcome", y = "Value", fill = "Outcome") +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    strip.text = element_text(size = 11),
    legend.position = "none"
  )
```


## By hemorrhage

```{r, fig.width=14, fig.height=12}
# Define the predictors of interest
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  CHOSEN_OUTCOME
)

# Reshape the data
df_long <-
  df_uni %>%
  select(all_of(cols)) %>%
  pivot_longer(
    cols = -c(CHOSEN_OUTCOME, `Hemorrhage at presentation`),
    names_to = "predictor",
    values_to = "value"
  )

# Plot the box plots
df_long %>%
  ggplot(aes_string(
    x = CHOSEN_OUTCOME,
    y = "value",
    fill = "`Hemorrhage at presentation`",
    color = "`Hemorrhage at presentation`"
  )) +
  geom_boxplot(alpha = 0.5) +
  facet_wrap(~predictor, scales = "free") +
  labs(x = "Outcome", y = "Value", fill = "Outcome") +
  theme_minimal() +
  guides(fill = "none") +
  theme(
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    strip.text = element_text(size = 11)
  )
```


## By SM grade high vs. low

```{r, fig.width=14, fig.height=12}
# Define the predictors of interest
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  CHOSEN_OUTCOME
)

# Reshape the data
df_long <-
  df_uni %>%
  select(all_of(cols)) %>%
  pivot_longer(
    cols = -c(CHOSEN_OUTCOME, `Spetzler-Martin grade < 4`),
    names_to = "predictor",
    values_to = "value"
  )

# Plot the box plots
df_long %>%
  ggplot(aes_string(
    x = CHOSEN_OUTCOME,
    y = "value",
    fill = "`Spetzler-Martin grade < 4`",
    color = "`Spetzler-Martin grade < 4`"
  )) +
  geom_boxplot(alpha = 0.5) +
  facet_wrap(~predictor, scales = "free") +
  labs(x = "Outcome", y = "Value", fill = "Outcome") +
  theme_minimal() +
  guides(fill = "none") +
  theme(
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    strip.text = element_text(size = 11)
  )
```


## Specialty

### SM grade vs. complications

```{r}
# Rename
complications <- c(
  has_complication_minor = "Minor",
  has_complication_major = "Major"
)

# Create table for plotting
t <- 
  df_uni %>% 
  filter(is_eligible) %>% 
  select(
    patient_id,
    spetzler_martin_grade,
    has_complication_minor,
    has_complication_major
  ) %>% 
  pivot_longer(starts_with("has_complication")) %>% 
  group_by(spetzler_martin_grade, name) %>% 
  summarize(
    num_patients = sum(value, na.rm = T)
  ) %>% 
  group_by(name) %>% 
  mutate(pct_patients = num_patients / sum(num_patients))

# Create figure
plots$sm_grade_vs_complications_bar_chart <- 
  t %>%
  mutate(name = factor(name, names(complications), complications)) %>% 
  ggplot(aes(
    x = spetzler_martin_grade,
    y = pct_patients,
    color = name,
    fill = name)
  ) +
  geom_col(
    position = position_dodge(width = 0.8),
    width = 0.7
  ) +
  geom_label(
    aes(label = num_patients),
    position = position_dodge(width = 0.8),
    vjust = -0.2,
    size = 3,
    fill = "white",
    label.size = NA,  # Removes the label border
    show.legend = FALSE
  ) +
  scale_y_continuous(
    labels = scales::percent_format(accuracy = 1),
    expand = expansion(mult = c(0, 0.1))
  ) +
  scale_fill_manual(values = c("Minor" = "#00BFC4", "Major" = "#F8766D")) +
  scale_color_manual(values = c("Minor" = "#00BFC4", "Major" = "#F8766D")) +
  labs(
    x = "Spetzler-Martin Grade",
    y = "Patients (%)",
    fill = "Complications",
    color = "Complications"
  ) +
  theme(
    legend.position = c(0.3, 0.95),
    legend.justification = c("right", "top")
  )
  
# Plot
plots$sm_grade_vs_complications_bar_chart
```


## By date

Every 1 year.

```{r}
# Create table
df_by_date <-
  df_uni %>%
  filter(!is.na(date_first_treatment)) %>% 
  mutate(year_first_treatment = format(date_first_treatment, "%Y")) %>% 
  group_by(year_first_treatment) %>% 
  summarize(
    num_patients = n(),
    num_poor_outcome = sum(is_poor_outcome),
    pct_poor_outcome = 100 * num_poor_outcome / num_patients
  )

df_by_date %>% 
  ggplot(aes(x = year_first_treatment, y = pct_poor_outcome)) +
  geom_col() +
  geom_text(
    aes(
      label = paste0(num_poor_outcome, "/", num_patients)
    ),
    size = 2,
    vjust = -0.5  # Adjust vertical position above the bar
  ) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1.5, vjust = 2.5)
  ) +
  labs(
    x = "Year of First Treatment",
    y = "Poor outcomes (%)",
    title = "Percentage of Poor Outcomes by Year"
  )
```


Every 5 years.

```{r}
# Create table
df_by_date <-
  df_uni %>%
  filter(!is.na(date_first_treatment)) %>% 
  mutate(year_first_treatment = as.numeric(format(date_first_treatment, "%Y"))) %>%
  mutate(
    year_interval = paste0(
      floor(year_first_treatment / 5) * 5,
      "-",
      floor(year_first_treatment / 5) * 5 + 4
    )
  ) %>% 
  group_by(year_interval) %>% 
  summarize(
    num_patients = n(),
    num_poor_outcome = sum(is_poor_outcome),
    pct_poor_outcome = 100 * num_poor_outcome / num_patients
  )

# Visualize
df_by_date %>% 
  ggplot(aes(x = year_interval, y = pct_poor_outcome)) +
  geom_col() +
  geom_text(
    aes(
      label = paste0(num_poor_outcome, "/", num_patients)
    ),
    size = 3,
    vjust = -0.5  # Adjust vertical position above the bar
  ) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 1)
  ) +
  labs(
    x = "Year of First Treatment",
    y = "Poor outcomes (%)",
    title = "Percentage of Poor Outcomes by Year"
  )
```


------------------------------------------------------------------------

# Descriptive statistics

## Setup

```{r}

```


## Non-specific

```{r}
# Rename dataframe
`Pediatric AVMs` <-
  df_uni %>%
  filter(is_eligible) %>%
  select(-is_eligible, -comments)

# Create summary
`Pediatric AVMs` %>%
  summarytools::dfSummary(display.labels = FALSE) %>%
  print(
    file =
      "../../outputs/descriptive-statistics/descriptive_statistics_eligible.html",
    footnote = NA
  )

# Remove unwanted dataframe
remove(`Pediatric AVMs`)
```


## By hemorrhage - table1

https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html

```{r eval=FALSE}
library(table1)

table1::table1(
  ~ factor(is_male) + age_at_first_treatment_yrs + factor(is_eloquent_location) + max_size_cm | has_hemorrhage,
  data = df_uni
)
```

```{r eval=FALSE}
# Initialize values
df <- df_uni

# Remove missing values in stratification variables
df <-
  df %>%
  drop_na(has_hemorrhage, CHOSEN_OUTCOME)

# Define columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  OUTCOMES[OUTCOMES == CHOSEN_OUTCOME]
)

# Assign labels to the variables in the dataframe
for (var in cols) {
  label(df[[var]]) <- names(cols[cols == var])
}

# Assign meaningful labels to the has_hemorrhage variable
df$has_hemorrhage <- factor(
  df$has_hemorrhage,
  levels = c(FALSE, TRUE),
  labels = c("No Hemorrhage", "Has Hemorrhage")
)

df[, CHOSEN_OUTCOME] <- factor(
  df %>% pull(CHOSEN_OUTCOME),
  levels = CHOSEN_OUTCOME_LEVELS,
  labels = CHOSEN_OUTCOME_LABELS
)

# Define custom rendering functions if needed
render_continuous <- function(x) {
  with(stats.apply.rounding(stats.default(x), digits = 2), c(
    "Mean (SD)" = sprintf("%s (&plusmn; %s)", MEAN, SD),
    "Median (IQR)" = sprintf("%s (%s - %s)", MEDIAN, Q1, Q3)
  ))
}

compute_pvalues <- function(x, ...) {
  # Construct vectors of data y, and groups (strata) g
  y <- unlist(x)
  g <- factor(rep(1:length(x), times = sapply(x, length)))
  if (is.numeric(y)) {
    # For numeric variables, perform a standard 2-sample t-test
    p <- t.test(y ~ g)$p.value
  } else {
    # For categorical variables, perform a chi-squared test of independence
    p <- chisq.test(table(y, g))$p.value
  }
  # Format the p-value, using an HTML entity for the less-than sign.
  # The initial empty string places the output on the line below the variable label.
  c("", sub("<", "&lt;", format.pval(p, digits = 3, eps = 0.001)))
}

# Create the descriptive table
frla <- as.formula(paste("~ . | has_hemorrhage *", CHOSEN_OUTCOME))

# Create table
table1_descriptive_statistics <-
  table1(frla,
    data = df[, unname(cols)],
    render.continuous = render_continuous,
    overall = c(left = "Overall"),
    topclass = "Rtable1-zebra"
  )

# Print
table1_descriptive_statistics
```


## By hemorrhage

```{r}
if (!str_detect(params$TITLE, "Hemorrhage")) {
  
  # Define arguments
  interaction_col <- "has_hemorrhage"
  interaction_col_name <- "Hemorrhage at presentation"
  prefix_true <- "Haem"
  prefix_false <- "No Haem"
  
  # Get all
  out <- list()
  out[[1]] <- compute_descriptive_continuous(
    use_interaction = F,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = "All"
  )
  out[[2]] <- compute_descriptive_binary(
    use_interaction = F,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = "All"
  )
  out_all <- bind_rows(out)
  
  # Get with hemorrhage
  out <- list()
  out[[1]] <- compute_descriptive_continuous(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = prefix_true
  )
  out[[2]] <- compute_descriptive_binary(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = prefix_true
  )
  out_haem <- bind_rows(out)
  
  # Get without hemorrhage
  out <- list()
  out[[1]] <- compute_descriptive_continuous(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = F,
    prefix = prefix_false
  )
  out[[2]] <- compute_descriptive_binary(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = F,
    prefix = prefix_false
  )
  out_no_haem <- bind_rows(out)
  
  # Synthesize
  out_complete <-
    out_all %>%
    left_join(out_haem, by = "keys") %>%
    left_join(out_no_haem, by = "keys") %>%
    arrange(`All - P-value`)
  
  # Print
  out_complete %>%
    sable()
  
}
```


## By diffuse nidus

```{r}
# Define arguments
interaction_col <- "is_diffuse_nidus"
interaction_col_name <- "Is diffuse nidus"
prefix_true <- "Diffuse"
prefix_false <- "Not Diffuse"

# Get all
out <- list()
out[[1]] <- compute_descriptive_continuous(
  use_interaction = F,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = T,
  prefix = "All"
)
out[[2]] <- compute_descriptive_binary(
  use_interaction = F,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = T,
  prefix = "All"
)
out_all <- bind_rows(out)

# Get with hemorrhage
out <- list()
out[[1]] <- compute_descriptive_continuous(
  use_interaction = T,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = T,
  prefix = prefix_true
)
out[[2]] <- compute_descriptive_binary(
  use_interaction = T,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = T,
  prefix = prefix_true
)
out_haem <- bind_rows(out)

# Get without hemorrhage
out <- list()
out[[1]] <- compute_descriptive_continuous(
  use_interaction = T,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = F,
  prefix = prefix_false
)
out[[2]] <- compute_descriptive_binary(
  use_interaction = T,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = F,
  prefix = prefix_false
)
out_no_haem <- bind_rows(out)

# Synthesize
out_complete <-
  out_all %>%
  left_join(out_haem, by = "keys") %>%
  left_join(out_no_haem, by = "keys") %>%
  arrange(`All - P-value`)

# Print
out_complete %>%
  sable()
```


## By SM grade high vs. low

```{r}
if (!str_detect(params$TITLE, "grade")) {
  # Define arguments
  interaction_col <- "is_spetzler_martin_grade_less_than_4"
  interaction_col_name <- "Spetzler-Martin grade < 4"
  prefix_true <- "Low grade"
  prefix_false <- "High grade"
  
  # Get all
  out <- list()
  out[[1]] <- compute_descriptive_continuous(
    use_interaction = F,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = "All"
  )
  out[[2]] <- compute_descriptive_binary(
    use_interaction = F,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = "All"
  )
  out_all <- bind_rows(out)
  
  # Get with
  out <- list()
  out[[1]] <- compute_descriptive_continuous(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = prefix_true
  )
  out[[2]] <- compute_descriptive_binary(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = T,
    prefix = prefix_true
  )
  out_haem <- bind_rows(out)
  
  # Get without
  out <- list()
  out[[1]] <- compute_descriptive_continuous(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = F,
    prefix = prefix_false
  )
  out[[2]] <- compute_descriptive_binary(
    use_interaction = T,
    interaction_col = interaction_col,
    interaction_col_name = interaction_col_name,
    interaction_col_value = F,
    prefix = prefix_false
  )
  out_no_haem <- bind_rows(out)
  
  # Synthesize
  out_complete <-
    out_all %>%
    left_join(out_haem, by = "keys") %>%
    left_join(out_no_haem, by = "keys") %>%
    arrange(`All - P-value`)
  
  # Print
  out_complete %>%
    sable()
}
```


## By date

```{r}
# Define arguments
outcome_col <- "is_first_tx_after_2009"
outcome_levels <- c(FALSE, TRUE)
outcome_labels <- c("After Jun '09", "At or before Jun '09")

# Get all
out <- list()
out[[1]] <- compute_descriptive_continuous(
  outcome_col = outcome_col,
  outcome_levels = outcome_levels,
  outcome_labels = outcome_labels,
  use_interaction = F,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = T,
  prefix = "All"
)
out[[2]] <- compute_descriptive_binary(
  cols = EXPOSURES_BINARY[!EXPOSURES_BINARY %in% c("is_first_tx_after_2009")],
  outcome_col = outcome_col,
  outcome_levels = outcome_levels,
  outcome_labels = outcome_labels,
  use_interaction = F,
  interaction_col = interaction_col,
  interaction_col_name = interaction_col_name,
  interaction_col_value = T,
  prefix = "All"
)
out_all <- bind_rows(out)

# Synthesize
out_complete <-
  out_all %>%
  arrange(`All - P-value`)

# Print
out_complete %>%
  sable()
```


------------------------------------------------------------------------

# Associational statistics

## Correlations

Correlation matrix.

```{r, fig.height=8, fig.width=8}
# Cols
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL,
  OUTCOMES
)

# Remove interaction terms
if (str_detect(params$TITLE, "Hemorrhage")) {
  cols <- cols[!cols == "has_hemorrhage"]
}
if (str_detect(params$TITLE, "grade")) {
  x <- c("spetzler_martin_grade", "is_spetzler_martin_grade_less_than_4")
  cols <- cols[!cols %in% x]
}
  
# Create new dataframe
df_ <-
  df_uni %>%
  select(all_of(cols))

# Convert the outcome variable to numeric for correlation calculation
df_ <-
  df_ %>%
  select(where(is.numeric), where(is.logical)) %>%
  mutate(across(everything(), as.numeric))

# Compute the correlation matrix
cor_matrix <- cor(df_, use = "complete.obs", method = "spearman")

# Plot the correlation matrix
ggcorrplot::ggcorrplot(
  cor_matrix,
  method = "circle",
  lab = TRUE,
  lab_size = 2,
  colors = c("red", "white", "green4"),
  title = "Correlation Matrix",
  hc.order = TRUE,
) +
  theme(
    axis.text.x = element_text(size = 8), # Adjust x-axis text size
    axis.text.y = element_text(size = 8) # Adjust y-axis text size
  )
```


------------------------------------------------------------------------

# Univariable statistics

## Setup

Define function.

```{r}
fit_model <- function(
    df = df_uni, cols = cols, is_sandwich = T) {
  # Initialize
  out <- list()

  for (i in seq_along(cols)) {
    # Create formula
    model <- as.formula(paste(CHOSEN_OUTCOME, "~", cols[i]))

    fit <- suppressWarnings(glm(
      model,
      data = df,
      family = binomial()
    ))

    if (is_sandwich) {
      # Calculate robust standard errors (sandwich)
      robust_se <- sandwich::vcovHC(fit, type = "HC0")
      # Compute robust confidence intervals
      fit_results <- lmtest::coeftest(fit, vcov. = robust_se)
    } else {
      fit_results <- fit
    }

    # Summarize model coefficients
    fit_summary <-
      fit_results %>%
      # broom does not support exponentiation after coeftest so do it manually
      broom::tidy(conf.int = T) %>%
      mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
      arrange(term == "(Intercept)", p.value) %>%
      rename(odds_ratio = estimate, z_value = statistic)

    # Stylize and print
    stylized <-
      fit_summary %>%
      rename(
        "Predictors" = term,
        "Odds Ratios (OR)" = odds_ratio,
        "SE" = std.error,
        "Z-scores" = z_value,
        "P-values" = p.value,
        "CI (low)" = conf.low,
        "CI (high)" = conf.high,
      ) %>%
      mutate(
        `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
        `SE` = round(`SE`, 2),
        `CI (low)` = round(`CI (low)`, 2),
        `CI (high)` = round(`CI (high)`, 2),
        `P-values` = round(`P-values`, 3),
      )

    out[[i]] <- stylized
  }
  return(out)
}
```


## Unadjusted - Original

Fit logistic with no adjustments. Use original feature definitions.

```{r}
# Define cols
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
))

# Fit model
out <- fit_model(df = df_uni, cols = cols, is_sandwich = T)

# Create table
univariable_unadjusted <-
  out %>%
  bind_rows() %>%
  filter(Predictors != "(Intercept)") %>%
  filter(`CI (high)` < 50) %>%
  arrange(`P-values`)

# Print
univariable_unadjusted %>%
  sable()
```


## Adjusted - Original

- Fit logistic
- Adjust for age at first treatment and sex
- Use original feature definitions

```{r}
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
))

# Initialize
out <- list()
df <- df_uni

for (i in seq_along(cols)) {
  # Create formula
  model <- as.formula(paste(
    CHOSEN_OUTCOME,
    "~",
    "age_at_first_treatment_yrs + is_male +",
    cols[i]
  ))

  fit <- suppressWarnings(glm(
    model,
    data = df,
    family = binomial()
  ))

  # Calculate robust standard errors (sandwich)
  robust_se <- sandwich::vcovHC(fit, type = "HC0")
  # Compute robust confidence intervals
  robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)

  # Summarize model coefficients
  fit_summary <-
    robust_ci %>%
    # broom does not support exponentiation after coeftest so do it manually
    broom::tidy(conf.int = T) %>%
    mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
    arrange(term == "(Intercept)", p.value) %>%
    rename(odds_ratio = estimate, z_value = statistic)

  # Stylize and print
  stylized <-
    fit_summary %>%
    rename(
      "Predictors" = term,
      "Odds Ratios (OR)" = odds_ratio,
      "SE" = std.error,
      "Z-scores" = z_value,
      "P-values" = p.value,
      "CI (low)" = conf.low,
      "CI (high)" = conf.high,
    ) %>%
    mutate(
      `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
      `SE` = round(`SE`, 2),
      `CI (low)` = round(`CI (low)`, 2),
      `CI (high)` = round(`CI (high)`, 2),
      `P-values` = round(`P-values`, 3),
    )

  out[[i]] <- stylized
}
```


Print results.

```{r}
# Create table
univariable_adjusted <-
  out %>%
  bind_rows() %>%
  filter(Predictors != "(Intercept)") %>%
  filter(!str_starts(Predictors, "age_at_first_treatment_yrs|is_male")) %>%
  filter(`CI (high)` < 50) %>%
  arrange(`P-values`)

# Print
univariable_adjusted %>%
  sable()
```


## Unadjusted - Recoded

- Fit logistic with no adjustments
- Use the recoded feature definitions (less levels for max statistical power)

```{r}
# Define columns
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
))

# Initialize
out <- list()
df <- df_multi

for (i in seq_along(cols)) {
  # Create formula
  model <- as.formula(paste(CHOSEN_OUTCOME, "~", cols[i]))

  fit <- suppressWarnings(glm(
    model,
    data = df,
    family = binomial()
  ))

  # Calculate robust standard errors (sandwich)
  robust_se <- sandwich::vcovHC(fit, type = "HC0")
  # Compute robust confidence intervals
  robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)

  # Summarize model coefficients
  fit_summary <-
    robust_ci %>%
    # broom does not support exponentiation after coeftest so do it manually
    broom::tidy(conf.int = T) %>%
    mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
    arrange(term == "(Intercept)", p.value) %>%
    rename(odds_ratio = estimate, z_value = statistic)

  # Stylize and print
  stylized <-
    fit_summary %>%
    rename(
      "Predictors" = term,
      "Odds Ratios (OR)" = odds_ratio,
      "SE" = std.error,
      "Z-scores" = z_value,
      "P-values" = p.value,
      "CI (low)" = conf.low,
      "CI (high)" = conf.high,
    ) %>%
    mutate(
      `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
      `SE` = round(`SE`, 2),
      `CI (low)` = round(`CI (low)`, 2),
      `CI (high)` = round(`CI (high)`, 2),
      `P-values` = round(`P-values`, 3),
    )

  out[[i]] <- stylized
}
```


Print results.

```{r}
# Create table
univariable_unadjusted_recoded <-
  out %>%
  bind_rows() %>%
  filter(Predictors != "(Intercept)") %>%
  filter(`CI (high)` < 50) %>%
  arrange(`P-values`)

# Print
univariable_unadjusted_recoded %>%
  sable()
```


## Adjusted - Recoded

- Fit logistic with no adjustments
- Adjust for age at first treatment and sex
- Use the recoded feature definitions (less levels for max statistical power)

```{r}
# Define columns
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
))

# Initialize
out <- list()
df <- df_multi

for (i in seq_along(cols)) {
  # Create formula
  model <- as.formula(paste(
    CHOSEN_OUTCOME,
    "~",
    "age_at_first_treatment_yrs + is_male +",
    cols[i]
  ))

  fit <- suppressWarnings(glm(
    model,
    data = df,
    family = binomial()
  ))

  # Calculate robust standard errors (sandwich)
  robust_se <- sandwich::vcovHC(fit, type = "HC0")
  # Compute robust confidence intervals
  robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)

  # Summarize model coefficients
  fit_summary <-
    robust_ci %>%
    # broom does not support exponentiation after coeftest so do it manually
    broom::tidy(conf.int = T) %>%
    mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
    arrange(term == "(Intercept)", p.value) %>%
    rename(odds_ratio = estimate, z_value = statistic)

  # Stylize and print
  stylized <-
    fit_summary %>%
    rename(
      "Predictors" = term,
      "Odds Ratios (OR)" = odds_ratio,
      "SE" = std.error,
      "Z-scores" = z_value,
      "P-values" = p.value,
      "CI (low)" = conf.low,
      "CI (high)" = conf.high,
    ) %>%
    mutate(
      `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
      `SE` = round(`SE`, 2),
      `CI (low)` = round(`CI (low)`, 2),
      `CI (high)` = round(`CI (high)`, 2),
      `P-values` = round(`P-values`, 3),
    )

  out[[i]] <- stylized
}
```


Print results.

```{r}
# Create table
univariable_adjusted_recoded <-
  out %>%
  bind_rows() %>%
  filter(Predictors != "(Intercept)") %>%
  filter(!str_starts(Predictors, "age_at_first_treatment_yrs|is_male")) %>%
  filter(`CI (high)` < 50) %>%
  arrange(`P-values`)

# Print
univariable_adjusted_recoded %>%
  sable()
```



------------------------------------------------------------------------

# Interaction analysis

## By hemorrhage

Fit logistic.

```{r}
# Define columns
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
))

# Remove unwanted columns
unwanted <- c("modified_rankin_score_final")
cols <- cols[!cols %in% unwanted]

adjustors <- c(
  "age_at_first_treatment_yrs",
  "is_male"
)

# Initialize
out <- list()
df <- df_multi
k <- 1

for (i in seq_along(cols)) {
  # Do not fit model for variables that we are adjusting for
  if (cols[i] %in% adjustors) {
    next
  }

  # Create formula
  model <- as.formula(paste(
    CHOSEN_OUTCOME,
    "~",
    "age_at_first_treatment_yrs + is_male +",
    cols[i],
    "* has_hemorrhage",
    sep = ""
  ))

  fit <- suppressWarnings(glm(
    model,
    data = df,
    family = binomial()
  ))

  # Calculate robust standard errors (sandwich)
  robust_se <- sandwich::vcovHC(fit, type = "HC0")
  # Compute robust confidence intervals
  robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)

  # Summarize model coefficients
  fit_summary <-
    robust_ci %>%
    # broom does not support exponentiation after coeftest so do it manually
    broom::tidy(conf.int = T) %>%
    mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
    arrange(term == "(Intercept)", p.value) %>%
    rename(odds_ratio = estimate, z_value = statistic)

  # Stylize and print
  stylized <-
    fit_summary %>%
    rename(
      "Predictors" = term,
      "Odds Ratios (OR)" = odds_ratio,
      "SE" = std.error,
      "Z-scores" = z_value,
      "P-values" = p.value,
      "CI (low)" = conf.low,
      "CI (high)" = conf.high,
    ) %>%
    mutate(
      `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
      `SE` = round(`SE`, 2),
      `CI (low)` = round(`CI (low)`, 2),
      `CI (high)` = round(`CI (high)`, 2),
      `P-values` = round(`P-values`, 3),
    )

  out[[k]] <- stylized
  k <- k + 1
}
```


Print all results results.

```{r}
# Define values
unwanted <- c(
  "is_maleTRUE",
  "age_at_first_treatment_yrs",
  "(Intercept)",
  "has_hemorrhageTRUE"
)

# Initialize objects
isolated_values <- list()

# Get main and interaction effects
for (i in seq_along(out)) {
  # Get data
  result <- out[[i]]

  # Get all main effects of interest
  main_effects <-
    result %>%
    filter(!Predictors %in% unwanted) %>%
    filter(!str_detect(Predictors, ":")) %>%
    select(-"SE", -"Z-scores")

  # Get the interaction effects
  interaction_effects <-
    result %>%
    filter(!Predictors %in% unwanted) %>%
    filter(str_detect(Predictors, ":")) %>%
    mutate(Predictors = str_extract(Predictors, "^[^:]+")) %>%
    select(-"SE", -"Z-scores") %>%
    rename_with(~ paste("Interaction -", .), -Predictors)

  isolated_values[[i]] <-
    main_effects %>%
    left_join(interaction_effects, by = "Predictors")
}
```


Create and print table

```{r}
# Create table
univariable_adjusted_recoded_interactions <-
  isolated_values %>%
  bind_rows() %>%
  arrange(`Interaction - P-values`)

# Print
univariable_adjusted_recoded_interactions %>%
  sable()
```


Eloquence.

```{r}
df %>%
  count(has_hemorrhage, is_eloquent_location, !!sym(CHOSEN_OUTCOME)) %>%
  drop_na() %>%
  arrange(has_hemorrhage, is_eloquent_location) %>%
  group_by(has_hemorrhage, is_eloquent_location) %>%
  mutate(
    num_total = sum(n),
    pct_total = scales::percent(n / num_total, 1)
  ) %>%
  sable()
```


Deficit.

```{r}
df %>%
  count(has_hemorrhage, has_deficit, !!sym(CHOSEN_OUTCOME)) %>%
  drop_na() %>%
  arrange(has_hemorrhage, has_deficit) %>%
  group_by(has_hemorrhage, has_deficit) %>%
  mutate(
    num_total = sum(n),
    pct_total = scales::percent(n / num_total, 1)
  ) %>%
  sable()
```


Seizures.

```{r}
df %>%
  count(has_hemorrhage, has_seizures, !!sym(CHOSEN_OUTCOME)) %>%
  drop_na() %>%
  arrange(has_hemorrhage, has_seizures) %>%
  group_by(has_hemorrhage, has_seizures) %>%
  mutate(
    num_total = sum(n),
    pct_total = scales::percent(n / num_total, 1)
  ) %>%
  sable()
```


## By SM grade high vs. low

Fit logistic.

```{r}
# Define columns
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
))

# Remove unwanted columns
unwanted <- c("modified_rankin_score_final")
cols <- cols[!cols %in% unwanted]

adjustors <- c(
  "age_at_first_treatment_yrs",
  "is_male"
)

# Initialize
out <- list()
df <- df_multi
k <- 1

for (i in seq_along(cols)) {
  # Do not fit model for variables that we are adjusting for
  if (cols[i] %in% adjustors) {
    next
  }

  # Create formula
  model <- as.formula(paste(
    CHOSEN_OUTCOME,
    "~",
    "age_at_first_treatment_yrs + is_male +",
    cols[i],
    "* is_spetzler_martin_grade_less_than_4",
    sep = ""
  ))

  fit <- suppressWarnings(glm(
    model,
    data = df,
    family = binomial()
  ))

  # Calculate robust standard errors (sandwich)
  robust_se <- sandwich::vcovHC(fit, type = "HC0")
  # Compute robust confidence intervals
  robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)

  # Summarize model coefficients
  fit_summary <-
    robust_ci %>%
    # broom does not support exponentiation after coeftest so do it manually
    broom::tidy(conf.int = T) %>%
    mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
    arrange(term == "(Intercept)", p.value) %>%
    rename(odds_ratio = estimate, z_value = statistic)

  # Stylize and print
  stylized <-
    fit_summary %>%
    rename(
      "Predictors" = term,
      "Odds Ratios (OR)" = odds_ratio,
      "SE" = std.error,
      "Z-scores" = z_value,
      "P-values" = p.value,
      "CI (low)" = conf.low,
      "CI (high)" = conf.high,
    ) %>%
    mutate(
      `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
      `SE` = round(`SE`, 2),
      `CI (low)` = round(`CI (low)`, 2),
      `CI (high)` = round(`CI (high)`, 2),
      `P-values` = round(`P-values`, 3),
    )

  out[[k]] <- stylized
  k <- k + 1
}
```


Print all results results.

```{r}
# Define values
unwanted <- c(
  "is_maleTRUE",
  "age_at_first_treatment_yrs",
  "(Intercept)",
  "is_spetzler_martin_grade_less_than_4TRUE"
)

# Initialize objects
isolated_values <- list()

# Get main and interaction effects
for (i in seq_along(out)) {
  # Get data
  result <- out[[i]]

  # Get all main effects of interest
  main_effects <-
    result %>%
    filter(!Predictors %in% unwanted) %>%
    filter(!str_detect(Predictors, ":")) %>%
    select(-"SE", -"Z-scores")

  # Get the interaction effects
  interaction_effects <-
    result %>%
    filter(!Predictors %in% unwanted) %>%
    filter(str_detect(Predictors, ":")) %>%
    mutate(Predictors = str_extract(Predictors, "^[^:]+")) %>%
    select(-"SE", -"Z-scores") %>%
    rename_with(~ paste("Interaction -", .), -Predictors)

  isolated_values[[i]] <-
    main_effects %>%
    left_join(interaction_effects, by = "Predictors")
}
```


Create and print table

```{r}
# Create table
univariable_adjusted_recoded_interactions <-
  isolated_values %>%
  bind_rows() %>%
  arrange(`Interaction - P-values`)

# Print
univariable_adjusted_recoded_interactions %>%
  sable()
```


Eloquence.

```{r}
df %>%
  count(
    is_spetzler_martin_grade_less_than_4,
    is_eloquent_location,
    !!sym(CHOSEN_OUTCOME)
  ) %>%
  drop_na() %>%
  arrange(is_spetzler_martin_grade_less_than_4, is_eloquent_location) %>%
  group_by(is_spetzler_martin_grade_less_than_4, is_eloquent_location) %>%
  mutate(
    num_total = sum(n),
    pct_total = scales::percent(n / num_total, 1)
  ) %>%
  sable()
```


Deficit.

```{r}
df %>%
  count(
    is_spetzler_martin_grade_less_than_4,
    has_deficit,
    !!sym(CHOSEN_OUTCOME)
  ) %>%
  drop_na() %>%
  arrange(is_spetzler_martin_grade_less_than_4, has_deficit) %>%
  group_by(is_spetzler_martin_grade_less_than_4, has_deficit) %>%
  mutate(
    num_total = sum(n),
    pct_total = scales::percent(n / num_total, 1)
  ) %>%
  sable()
```


Seizures.

```{r}
df %>%
  count(
    is_spetzler_martin_grade_less_than_4,
    has_seizures,
    !!sym(CHOSEN_OUTCOME)
  ) %>%
  drop_na() %>%
  arrange(is_spetzler_martin_grade_less_than_4, has_seizures) %>%
  group_by(is_spetzler_martin_grade_less_than_4, has_seizures) %>%
  mutate(
    num_total = sum(n),
    pct_total = scales::percent(n / num_total, 1)
  ) %>%
  sable()
```


------------------------------------------------------------------------

# Selective inference

Read the following guides:

- [How to use glmnet](https://glmnet.stanford.edu/articles/glmnet.html)

- [Selective inference using forward selection](https://stephens999.github.io/misc/selective_inference_toy.html)


## Setup

Define unwanted columns.

```{r}
unwanted_all <- c(
  # Use values at or before first treatment
  "modified_rankin_score_postop_within_1_week",
  "modified_rankin_score_final",
  # Use the split between high vs. low grade instead of the granular
  "spetzler_martin_grade",
  # Irrelevant to treatment
  "is_first_tx_after_2009"
)

# Remove interaction terms from the mix
if (str_detect(params$TITLE, "Hemorrhage")) {
  unwanted_all <- c(unwanted_all, "has_hemorrhage")
}
if (str_detect(params$TITLE, "grade")) {
  x <- c("spetzler_martin_grade", "is_spetzler_martin_grade_less_than_4")
  unwanted_all <- c(unwanted_all, x)
}

unwanted_without_gradings <- c(
  unwanted_all,
  # Spetzler-Martin grade (size score + eloquent location + venous drainage)
  "is_spetzler_martin_grade_less_than_4",
  "size_score", # Already covered by the more detailed max_size_cm
  "venous_drainage", # Already covered by has_deep_venous_drainage
  "location", # Already covered to some extent by eloquence
  # Lawton-Young grade (SM + age + diffuse nidus + hemorrhage)
  # (SM is heavily overlapping with LY, so include its components)
  "lawton_young_grade",
  # Use mRS pre-treatment (more predictive)
  "modified_rankin_score_presentation" # "modified_rankin_score_pretreatment"
)

# Define cols of interest - use grading scores whenever these exist
unwanted_with_gradings <- c(
  unwanted_all,
  # Spetzler-Martin grade (size score + eloquent location + venous drainage)
  "is_spetzler_martin_grade_less_than_4", # The grade has lower p-value
  "size_score",
  "max_size_cm",
  "is_eloquent_location",
  # "location",  # Include this as not highly correlated with SM
  "has_deep_venous_drainage",
  "venous_drainage",
  # Lawton-Young grade (SM + age + diffuse nidus + hemorrhage)
  # (SM is heavily overlapping with LY, so include its components)
  "lawton_young_grade",
  # "is_diffuse_nidus",
  # "has_hemorrhage",
  # Use values at or before first treatment
  "modified_rankin_score_final",
  "modified_rankin_score_postop_within_1_week",
  # Use mRS pre-treatment (more predictive)
  "modified_rankin_score_presentation"
  # "modified_rankin_score_pretreatment"
)
```


Create dataset.

```{r}
# Define cols
cols <- unname(c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL,
  CHOSEN_OUTCOME
))

# Define column sets for each analysis
cols_all <- cols[!cols %in% unwanted_all]
cols_with_gradings <- cols[!cols %in% unwanted_with_gradings]
cols_without_gradings <- cols[!cols %in% unwanted_without_gradings]

# Create df of interest
df_all <-
  # Use the non-recoded dataset
  df_multi %>%
  dplyr::select(all_of(cols_all)) %>%
  drop_na()

df_with_gradings <-
  # Use the non-recoded dataset
  df_multi %>%
  dplyr::select(all_of(cols_with_gradings)) %>%
  drop_na()

df_no_scores <-
  # Use the non-recoded dataset
  df_multi %>%
  dplyr::select(all_of(cols_without_gradings)) %>%
  drop_na()

# Create formula
frla <- as.formula(paste(CHOSEN_OUTCOME, " ~ . - 1"))

# Create matrices with all variables
# (remove falsely created FALSE columns)
X_ <- model.matrix(frla, df_all)
X_all <- X_[, !grepl("FALSE$", colnames(X_))]
y_all <- df_all %>%
  pull(CHOSEN_OUTCOME) %>%
  as.numeric()

# Create matrices with scores
X_ <- model.matrix(frla, df_with_gradings)
X_with_gradings <- X_[, !grepl("FALSE$", colnames(X_))]
y_with_gradings <- df_with_gradings %>%
  pull(CHOSEN_OUTCOME) %>%
  as.numeric()

# Create matrices with score components
X_ <- model.matrix(frla, df_no_scores)
X_without_gradings <- X_[, !grepl("FALSE$", colnames(X_))]
y_without_gradings <- df_no_scores %>%
  pull(CHOSEN_OUTCOME) %>%
  as.numeric()


# Remove duplicate columns
# Define priority pairs (keep first, remove second)
priority_pairs <- list(
  c("is_maleTRUE", "procedure_combinationsS")
  # Add more pairs as needed
)

# Remove lower priority columns when both exist
for (pair in priority_pairs) {
  if(all(pair %in% colnames(X_all))) {
    cols_ <- colnames(X_all)
    X_all <- X_all[, !cols_ %in% pair[2]]
  }
  if(all(pair %in% colnames(X_with_gradings))) {
    cols_ <- colnames(X_with_gradings)
    X_with_gradings <- X_with_gradings[, !cols_ %in% pair[2]]
  }
  if(all(pair %in% colnames(X_without_gradings))) {
    cols_ <- colnames(X_without_gradings)
    X_without_gradings <- X_without_gradings[, !cols_ %in% pair[2]]
  }
}


# X should be centered for LASSO (necessary for glmnet)
# (scale = FALSE as this is done by the standardization step in glmnet)
# whe
center_values <- function(X) {
  # Identify binary columns (those with only 0s and 1s)
  binary_cols <- apply(X, 2, function(x) all(x %in% c(0, 1)))
  # Center but do not scale - avoid centering one-hot encoded vectors
  X[, !binary_cols] <- scale(X[, !binary_cols], center = T, scale = F)
  # Return
  return(X)
}

X_all_centered <- center_values(X_all)
X_with_gradings_centered <- center_values(X_with_gradings)
X_without_gradings_centered <- center_values(X_without_gradings)

# See the names of X to match column numbers to column names later on
colnames(X_without_gradings_centered)
```


## Stepwise

Use step-wise linear regression methods.

```{r}
# Set seed
set.seed(33)

# Run forward step-wise
fsfit <- selectiveInference::fs(
  x = X_without_gradings_centered,
  y = y_without_gradings,
  maxsteps = 2000,
  intercept = TRUE,
  normalize = FALSE
)

# Estimate sigma
sigmahat <- selectiveInference::estimateSigma(
  x = X_without_gradings_centered,
  y = y_without_gradings,
  intercept = TRUE,
  standardize = FALSE
)$sigmahat
```

```{r}
# Compute sequential p-values and confidence intervals - for all
# (sigma estimated from full model)
out.seq <- fsInf(fsfit, type = "active", sigma = sigmahat)
out.seq
```

```{r}
# Compute p-values and confidence intervals after AIC stopping - for selected
out.aic <- fsInf(fsfit, type = "aic", sigma = sigmahat)
out.aic
```

```{r}
# Compute p-values and confidence intervals after 5 fixed steps
out.fix <- fsInf(fsfit, type = "all", k = 5, sigma = sigmahat)
out.fix
```


## LASSO - all

Use LASSO logistic regression using all available variables.

```{r}
# Set seed
set.seed(141845)

# Define values
X <- X_all_centered
y <- y_all

# Perform cross-validation
# (alpha = 1 for lasso, alpha = 0 for ridge, 0 < alpha < 1 for elastic net)
cv_fit <- cv.glmnet(
  x = X,
  y = y,
  family = "binomial",
  alpha = 1,
  nfolds = 10
)

# Extract the best lambda
best_lambda_min <- cv_fit$lambda.min
best_lambda_1se <- cv_fit$lambda.1se

# Run glmnet
gfit <- glmnet(
  x = X,
  y = y,
  family = "binomial",
  alpha = 1,
  intercept = TRUE,
  standardize = TRUE
)

# Choose lambda
idx <- which(cv_fit$lambda %in% best_lambda_min)
num_coefs <- as.numeric(cv_fit$nzero[idx])

if (num_coefs == 0) {
  # Go one up to get more coefficients
  lambda <- cv_fit$lambda[idx + 3]
} else {
  # Given the small number of observations hence large SD, use min lambda
  lambda <- best_lambda_min
}

# Extract coef for a given lambda - avoid the intercept term
lambda <- best_lambda_min
n <- nrow(y)
beta <- coef(
  x = X,
  y = y,
  object = gfit,
  s = lambda,
  exact = TRUE
)

# Estimate sigma
gsigmahat <- estimateSigma(
  x = X,
  y = y,
  intercept = TRUE,
  standardize = TRUE
)$sigmahat

# Compute fixed lambda p-values and selection intervals
out <- fixedLassoInf(
  x = X,
  y = y,
  beta = beta,
  lambda = lambda,
  # Level of significance
  alpha = 0.1,
  family = "binomial",
  intercept = TRUE,
  sigma = gsigmahat
)

# Create model table (make this robust to no variable being selected)
if (length(out) > 0) {

  # Create table
  robust_ci <- 
    tibble(
      term = names(out$vars),
      odds_ratio = exp(out$coef0),
      pvalue = out$pv,
      ci_lo = exp(out$ci[, 1]),
      ci_hi = exp(out$ci[, 2])
    ) %>%
    arrange(pvalue)
  
  # Print
  robust_ci %>% sable()

} else {
  
  # Print 
  print("No variables were selected")
}
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(out) > 0) {
  # Create table
  coef_table <-
    robust_ci %>% 
    filter(term != "(Intercept)") %>%
    arrange(desc(odds_ratio)) %>%
    mutate(
      term = prettify_terms(term, cols),
      term = factor(term, levels = term)
    ) %>%
    # Cup the high confidence interval to make the figure readable
    mutate(ci_hi = ifelse(ci_hi > 128, 128, ci_hi))
  
  # Plot
  odds_ratios_plot <-
    coef_table %>%
    ggplot(aes(x = odds_ratio, y = term, color = term)) +
    geom_point() +
    geom_errorbar(aes(xmin = ci_lo, xmax = ci_hi), width = 0.1) +
    geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
    labs(x = "Odds Ratio", y = NULL) +
    guides(color = FALSE) +
    scale_x_continuous(
      trans = "log2",
      breaks = scales::trans_breaks("log2", function(x) 2^x),
      labels = function(x) formatC(x, format = "g", digits = 2)
      # labels = scales::label_number(accuracy = 0.1, trim = TRUE)
    ) +
    theme(
      axis.text = element_text(size = 12),
      axis.title = element_text(size = 13),
      plot.margin = unit(c(1, 0, 1, 0), "lines")  # Reduce right margin
    )
  
  pvalues_table_plot <- 
    coef_table %>% 
    mutate(pvalue = 
             formatC(pvalue, digits = 2, format = "fg", drop0trailing = F)) %>% 
    ggplot(aes(x = 1, y = term)) +
    geom_text(aes(label = pvalue), hjust = 0, size = 4) +
    scale_y_discrete(limits = levels(coef_table$term)) +
    theme_void() +
    ggtitle("P-value") +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.margin = unit(c(1, 0, 1, 0), "lines"),  # Reduce left margin
      plot.title = element_text(hjust = 0.75, face = "bold")
    )
  
  plots$odds_ratios_all_predictors <-
    odds_ratios_plot +
    plot_spacer() +
    pvalues_table_plot + 
    plot_layout(ncol = 3, widths = c(1, -0.19, 0.35))
  
  # Plot
  plots$odds_ratios_all_predictors
}
```


## LASSO - without scores

Use LASSO logistic regression not based on previous gradings.

```{r}
# Set seed
set.seed(141845)

# Define values
X <- X_without_gradings_centered
y <- y_without_gradings

# Perform cross-validation
# (alpha = 1 for lasso, alpha = 0 for ridge, 0 < alpha < 1 for elastic net)
cv_fit <- cv.glmnet(
  x = X,
  y = y,
  family = "binomial",
  alpha = 1,
  nfolds = 10
)

# Extract the best lambda
best_lambda_min <- cv_fit$lambda.min
best_lambda_1se <- cv_fit$lambda.1se

# Run glmnet
gfit <- glmnet(
  x = X,
  y = y,
  family = "binomial",
  alpha = 1,
  intercept = TRUE,
  standardize = TRUE
)

# Choose lambda
idx <- which(cv_fit$lambda %in% best_lambda_min)
num_coefs <- as.numeric(cv_fit$nzero[idx])

if (num_coefs == 0) {
  # Go one up to get more coefficients
  lambda <- cv_fit$lambda[idx + 3]
} else {
  # Given the small number of observations hence large SD, use min lambda
  lambda <- best_lambda_min
}

# Extract coef for a given lambda - avoid the intercept term
n <- nrow(y)
beta <- coef(
  x = X,
  y = y,
  object = gfit,
  s = lambda,
  exact = TRUE
)

# Estimate sigma
gsigmahat <- estimateSigma(
  x = X,
  y = y,
  intercept = TRUE,
  standardize = TRUE
)$sigmahat

# Compute fixed lambda p-values and selection intervals
out <- fixedLassoInf(
  x = X,
  y = y,
  beta = beta,
  lambda = lambda,
  # Level of significance
  alpha = 0.1,
  family = "binomial",
  intercept = TRUE,
  sigma = gsigmahat
)

# Create model table (make this robust to no variable being selected)
if (length(out) > 0) {

  # Create table
  robust_ci <- 
    tibble(
      term = names(out$vars),
      odds_ratio = exp(out$coef0),
      pvalue = out$pv,
      ci_lo = exp(out$ci[, 1]),
      ci_hi = exp(out$ci[, 2])
    ) %>%
    arrange(pvalue)
  
  # Print
  robust_ci %>% sable()

} else {

  # Print 
  print("No variables were selected")
}
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(out) > 0) {
  # Create table
  coef_table <-
    robust_ci %>% 
    filter(term != "(Intercept)") %>%
    arrange(desc(odds_ratio)) %>%
    mutate(
      term = prettify_terms(term, cols),
      term = factor(term, levels = term)
    ) %>%
    # Cup the high confidence interval to make the figure readable
    mutate(ci_hi = ifelse(ci_hi > 128, 128, ci_hi))
  
  # Plot
  odds_ratios_plot <-
    coef_table %>%
    ggplot(aes(x = odds_ratio, y = term, color = term)) +
    geom_point() +
    geom_errorbar(aes(xmin = ci_lo, xmax = ci_hi), width = 0.1) +
    geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
    labs(x = "Odds Ratio", y = NULL) +
    guides(color = FALSE) +
    scale_x_continuous(
      trans = "log2",
      breaks = scales::trans_breaks("log2", function(x) 2^x),
      labels = function(x) formatC(x, format = "g", digits = 2)
      # labels = scales::label_number(accuracy = 0.1, trim = TRUE)
    ) +
    theme(
      axis.text = element_text(size = 12),
      axis.title = element_text(size = 13),
      plot.margin = unit(c(1, 0, 1, 0), "lines")  # Reduce right margin
    )
  
  pvalues_table_plot <- 
    coef_table %>% 
    mutate(pvalue = 
             formatC(pvalue, digits = 2, format = "fg", drop0trailing = F)) %>% 
    ggplot(aes(x = 1, y = term)) +
    geom_text(aes(label = pvalue), hjust = 0, size = 4) +
    scale_y_discrete(limits = levels(coef_table$term)) +
    theme_void() +
    ggtitle("P-value") +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.margin = unit(c(1, 0, 1, 0), "lines"),  # Reduce left margin
      plot.title = element_text(hjust = 0.75, face = "bold")
    )
  
  plots$odds_ratios_with_components <-
    odds_ratios_plot +
    plot_spacer() +
    pvalues_table_plot + 
    plot_layout(ncol = 3, widths = c(1, -0.19, 0.35))
  
  # Plot
  plots$odds_ratios_with_components
}
```


## LASSO - without components

Use LASSO logistic regression based on previous gradings.

```{r}
# Set seed
set.seed(141845)

# Define values
X <- X_with_gradings_centered
y <- y_with_gradings

# Perform cross-validation
# (alpha = 1 for lasso, alpha = 0 for ridge, 0 < alpha < 1 for elastic net)
cv_fit <- cv.glmnet(
  x = X,
  y = y,
  family = "binomial",
  alpha = 1,
  nfolds = 10
)

# Extract the best lambda
best_lambda_min <- cv_fit$lambda.min
best_lambda_1se <- cv_fit$lambda.1se

# Run glmnet
gfit <- glmnet(
  x = X,
  y = y,
  family = "binomial",
  alpha = 1,
  intercept = TRUE,
  standardize = TRUE
)

# Choose lambda
idx <- which(cv_fit$lambda %in% best_lambda_min)
num_coefs <- as.numeric(cv_fit$nzero[idx])

if (num_coefs == 0) {
  # Go one up to get more coefficients
  lambda <- cv_fit$lambda[idx + 3]
} else {
  # Given the small number of observations hence large SD, use min lambda
  lambda <- best_lambda_min
}

# Extract coef for a given lambda - avoid the intercept term
lambda <- best_lambda_min
n <- nrow(y)
beta <- coef(
  x = X,
  y = y,
  object = gfit,
  s = lambda,
  exact = TRUE
)

# Estimate sigma
gsigmahat <- estimateSigma(
  x = X,
  y = y,
  intercept = TRUE,
  standardize = TRUE
)$sigmahat

# Compute fixed lambda p-values and selection intervals
out <- fixedLassoInf(
  x = X,
  y = y,
  beta = beta,
  lambda = lambda,
  # Level of significance
  alpha = 0.1,
  family = "binomial",
  intercept = TRUE,
  sigma = gsigmahat
)

# Create model table (make this robust to no variable being selected)
if (length(out) > 0) {

  # Create table
  robust_ci <- 
    tibble(
      term = names(out$vars),
      odds_ratio = exp(out$coef0),
      pvalue = out$pv,
      ci_lo = exp(out$ci[, 1]),
      ci_hi = exp(out$ci[, 2])
    ) %>%
    arrange(pvalue)
  
  # Print
  robust_ci %>% sable()

} else {

  # Print 
  print("No variables were selected")
}

# Print
robust_ci %>% sable()
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(out) > 0) {
  # Create table
  coef_table <-
    robust_ci %>% 
    filter(term != "(Intercept)") %>%
    arrange(desc(odds_ratio)) %>%
    mutate(
      term = prettify_terms(term, cols),
      term = factor(term, levels = term)
    ) %>%
    # Cup the high confidence interval to make the figure readable
    mutate(ci_hi = ifelse(ci_hi > 128, 128, ci_hi))
  
  # Plot
  odds_ratios_plot <-
    coef_table %>%
    ggplot(aes(x = odds_ratio, y = term, color = term)) +
    geom_point() +
    geom_errorbar(aes(xmin = ci_lo, xmax = ci_hi), width = 0.1) +
    geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
    labs(x = "Odds Ratio", y = NULL) +
    guides(color = FALSE) +
    scale_x_continuous(
      trans = "log2",
      breaks = scales::trans_breaks("log2", function(x) 2^x),
      labels = function(x) formatC(x, format = "g", digits = 2)
      # labels = scales::label_number(accuracy = 0.1, trim = TRUE)
    ) +
    theme(
      axis.text = element_text(size = 12),
      axis.title = element_text(size = 13),
      plot.margin = unit(c(1, 0, 1, 0), "lines")  # Reduce right margin
    )
  
  pvalues_table_plot <- 
    coef_table %>% 
    mutate(pvalue = 
             formatC(pvalue, digits = 2, format = "fg", drop0trailing = F)) %>% 
    ggplot(aes(x = 1, y = term)) +
    geom_text(aes(label = pvalue), hjust = 0, size = 4) +
    scale_y_discrete(limits = levels(coef_table$term)) +
    theme_void() +
    ggtitle("P-value") +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.margin = unit(c(1, 0, 1, 0), "lines"),  # Reduce left margin
      plot.title = element_text(hjust = 0.75, face = "bold")
    )
  
  plots$odds_ratios_without_components <-
    odds_ratios_plot +
    plot_spacer() +
    pvalues_table_plot + 
    plot_layout(ncol = 3, widths = c(1, -0.19, 0.35))
  
  # Plot
  plots$odds_ratios_without_components
}
```


## LASSO - without scores - without surgery

- Use LASSO logistic regression not based on previous gradings.
- Exclude surgery from the mix

```{r}
# Set seed
set.seed(141845)

# Define values
X <- X_without_gradings_centered
y <- y_without_gradings

if (any(colnames(X) %in% "is_surgeryTRUE")) {
  
  X <- X[, !colnames(X) %in% "is_surgeryTRUE"]
  
  # Perform cross-validation
  # (alpha = 1 for lasso, alpha = 0 for ridge, 0 < alpha < 1 for elastic net)
  cv_fit <- cv.glmnet(
    x = X,
    y = y,
    family = "binomial",
    alpha = 1,
    nfolds = 10
  )
  
  # Extract the best lambda
  best_lambda_min <- cv_fit$lambda.min
  best_lambda_1se <- cv_fit$lambda.1se
  
  # Run glmnet
  gfit <- glmnet(
    x = X,
    y = y,
    family = "binomial",
    alpha = 1,
    intercept = TRUE,
    standardize = TRUE
  )
  
  # Choose lambda
  idx <- which(cv_fit$lambda %in% best_lambda_min)
  num_coefs <- as.numeric(cv_fit$nzero[idx])
  
  if (num_coefs == 0) {
    # Go one up to get more coefficients
    lambda <- cv_fit$lambda[idx + 3]
  } else {
    # Given the small number of observations hence large SD, use min lambda
    lambda <- best_lambda_min
  }
  
  # Extract coef for a given lambda - avoid the intercept term
  lambda <- best_lambda_min
  n <- nrow(y)
  beta <- coef(
    x = X,
    y = y,
    object = gfit,
    s = lambda,
    exact = TRUE
  )
  
  # Estimate sigma
  gsigmahat <- estimateSigma(
    x = X,
    y = y,
    intercept = TRUE,
    standardize = TRUE
  )$sigmahat
  
  # Compute fixed lambda p-values and selection intervals
  out <- fixedLassoInf(
    x = X,
    y = y,
    beta = beta,
    lambda = lambda,
    # Level of significance
    alpha = 0.1,
    family = "binomial",
    intercept = TRUE,
    sigma = gsigmahat
  )
  
  # Create model table (make this robust to no variable being selected)
  if (length(out) > 0) {
  
    # Create table
    robust_ci <- 
      tibble(
        term = names(out$vars),
        odds_ratio = exp(out$coef0),
        pvalue = out$pv,
        ci_lo = exp(out$ci[, 1]),
        ci_hi = exp(out$ci[, 2])
      ) %>%
      arrange(pvalue)
    
    # Print
    robust_ci %>% sable()
  
  } else {
  
    # Print 
    print("No variables were selected")
  }
  
}
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(out) > 0) {
  # Create table
  coef_table <-
    robust_ci %>% 
    filter(term != "(Intercept)") %>%
    arrange(desc(pmin(pvalue, 0.1)), desc(odds_ratio)) %>%
    mutate(
      term = prettify_terms(term, cols),
      term = factor(term, levels = term)
    ) %>%
    # Cup the high confidence interval to make the figure readable
    mutate(ci_hi = ifelse(ci_hi > 128, 128, ci_hi))
  
  # Plot
  odds_ratios_plot <-
    coef_table %>%
    ggplot(aes(x = odds_ratio, y = term, color = term)) +
    geom_point() +
    geom_errorbar(aes(xmin = ci_lo, xmax = ci_hi), width = 0.1) +
    geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
    labs(x = "Odds Ratio", y = NULL) +
    guides(color = FALSE) +
    scale_x_continuous(
      trans = "log2",
      breaks = scales::trans_breaks("log2", function(x) 2^x),
      labels = function(x) formatC(x, format = "g", digits = 2)
      # labels = scales::label_number(accuracy = 0.1, trim = TRUE)
    ) +
    theme(
      axis.text = element_text(size = 12),
      axis.title = element_text(size = 13),
      plot.margin = unit(c(1, 0, 1, 0), "lines")  # Reduce right margin
    )
  
  pvalues_table_plot <- 
    coef_table %>% 
    mutate(pvalue = formatC(pvalue, digits = 2, format = "fg")) %>% 
    ggplot(aes(x = 1, y = term)) +
    geom_text(aes(label = pvalue), hjust = 0, size = 4) +
    scale_y_discrete(limits = levels(coef_table$term)) +
    theme_void() +
    ggtitle("P-value") +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.margin = unit(c(1, 0, 1, 0), "lines"),  # Reduce left margin
      plot.title = element_text(hjust = 0.75, face = "bold")
    )
  
  plots$odds_ratios_with_components_no_surgery <-
    odds_ratios_plot +
    plot_spacer() +
    pvalues_table_plot + 
    plot_layout(ncol = 3, widths = c(1, -0.19, 0.35))
  
  # Plot
  plots$odds_ratios_with_components_no_surgery
}
```


## LASSO - without components - without surgery

- Use LASSO logistic regression based on previous gradings
- Exclude surgery from the mix because it's too predictive

```{r}
# Set seed
set.seed(141845)

# Define values
X <- X_with_gradings_centered
y <- y_without_gradings

if (any(colnames(X) %in% "is_surgeryTRUE")) {
  
  X <- X[, !colnames(X) %in% "is_surgeryTRUE"]
  
  # Perform cross-validation
  # (alpha = 1 for lasso, alpha = 0 for ridge, 0 < alpha < 1 for elastic net)
  cv_fit <- cv.glmnet(
    x = X,
    y = y,
    family = "binomial",
    alpha = 1,
    nfolds = 10
  )
  
  # Extract the best lambda
  best_lambda_min <- cv_fit$lambda.min
  best_lambda_1se <- cv_fit$lambda.1se
  
  # Run glmnet
  gfit <- glmnet(
    x = X,
    y = y,
    family = "binomial",
    alpha = 1,
    intercept = TRUE,
    standardize = TRUE
  )
  
  # Choose lambda
  idx <- which(cv_fit$lambda %in% best_lambda_min)
  num_coefs <- as.numeric(cv_fit$nzero[idx])
  
  if (num_coefs == 0) {
    # Go up to get more coefficients
    lambda <- cv_fit$lambda[idx + 3]
  } else {
    # Given the small number of observations hence large SD, use min lambda
    lambda <- best_lambda_min
  }
  
  # Extract coef for a given lambda - avoid the intercept term
  lambda <- best_lambda_min
  n <- nrow(y)
  beta <- coef(
    x = X,
    y = y,
    object = gfit,
    s = lambda,
    exact = TRUE
  )
  
  # Estimate sigma
  gsigmahat <- estimateSigma(
    x = X,
    y = y,
    intercept = TRUE,
    standardize = TRUE
  )$sigmahat
  
  # Compute fixed lambda p-values and selection intervals
  out <- fixedLassoInf(
    x = X,
    y = y,
    beta = beta,
    lambda = lambda,
    # Level of significance
    alpha = 0.1,
    family = "binomial",
    intercept = TRUE,
    sigma = gsigmahat
  )
  
  # Create model table (make this robust to no variable being selected)
  if (length(out) > 0) {
  
    # Create table
    robust_ci <- 
      tibble(
        term = names(out$vars),
        odds_ratio = exp(out$coef0),
        pvalue = out$pv,
        ci_lo = exp(out$ci[, 1]),
        ci_hi = exp(out$ci[, 2])
      ) %>%
      arrange(pvalue)
    
    # Print
    robust_ci %>% sable()
  
  } else {
  
    # Print 
    print("No variables were selected")
  }
  
}
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(out) > 0) {
  # Create table
  coef_table <-
    robust_ci %>% 
    filter(term != "(Intercept)") %>%
    arrange(desc(pmin(pvalue, 0.1)), desc(odds_ratio)) %>%
    mutate(
      term = prettify_terms(term, cols),
      term = factor(term, levels = term)
    ) %>%
    # Cup the high confidence interval to make the figure readable
    mutate(ci_hi = ifelse(ci_hi > 128, 128, ci_hi))
  
  # Plot
  odds_ratios_plot <-
    coef_table %>%
    ggplot(aes(x = odds_ratio, y = term, color = term)) +
    geom_point() +
    geom_errorbar(aes(xmin = ci_lo, xmax = ci_hi), width = 0.1) +
    geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
    labs(x = "Odds Ratio", y = NULL) +
    guides(color = FALSE) +
    scale_x_continuous(
      trans = "log2",
      breaks = scales::trans_breaks("log2", function(x) 2^x),
    #   labels = function(x) {
    #   sapply(x, function(val) {
    #     if (is.na(val)) {
    #       NA  # Keep NA values as they are
    #     } else if (val >= 0.0625) {
    #       formatC(val, format = "fg", digits = 2)
    #     } else {
    #       formatC(val, format = "e", digits = 1)
    #     }
    #   })
    # }
      labels = function(x) formatC(x, format = "g", digits = 2)
      # labels = scales::label_number(accuracy = 0.1, trim = TRUE)
    ) +
    theme(
      axis.text = element_text(size = 12),
      axis.title = element_text(size = 13),
      plot.margin = unit(c(1, 0, 1, 0), "lines")  # Reduce right margin
    )
  
  pvalues_table_plot <- 
    coef_table %>% 
    mutate(pvalue = formatC(pvalue, digits = 2, format = "fg")) %>% 
    ggplot(aes(x = 1, y = term)) +
    geom_text(aes(label = pvalue), hjust = 0, size = 4) +
    scale_y_discrete(limits = levels(coef_table$term)) +
    theme_void() +
    ggtitle("P-value") +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.margin = unit(c(1, 0, 1, 0), "lines"),  # Reduce left margin
      plot.title = element_text(hjust = 0.75, face = "bold")
    )
  
  plots$odds_ratios_without_components_no_surgery <-
    odds_ratios_plot +
    plot_spacer() +
    pvalues_table_plot + 
    plot_layout(ncol = 3, widths = c(1, -0.19, 0.35))
  
  # Plot
  plots$odds_ratios_without_components_no_surgery
}
```

------------------------------------------------------------------------

# Multivariable - Without scores

## Feature selection

Create predictors.

```{r}
# Define p-value threshold
pval_threshold <- 0.2

# Get the predictors of interest
predictors <-
  univariable_adjusted_recoded %>%
  bind_rows(univariable_unadjusted_recoded) %>%
  filter(`P-values` < pval_threshold) %>%
  pull(`Predictors`)

# Clean categorical variables
predictors <-
  predictors %>%
  str_replace("^([a-z0-9_]*)([A-Z].*)$", "\\1") %>%
  unique()

# Add adjustment variables
predictors <- unique(c(predictors))

# Remove unwanted predictors
predictors <- predictors[!predictors %in% unwanted_without_gradings]

# Print
print(predictors)
```


## Model selection

Fit logistic.

```{r}
# Define data
df <- df_multi %>% drop_na(modified_rankin_score_pretreatment, location)

# Create formula
model <- as.formula(paste(
  CHOSEN_OUTCOME,
  "~",
  paste(predictors, collapse = " + ")
))

fit <- glm(
  model,
  data = df,
  family = binomial()
)

# Save
fit_without_grading <- fit
```


Print results.

```{r}
# Summarize model coefficients
fit_summary <-
  fit %>%
  broom::tidy(exponentiate = T, conf.int = T) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
stylized <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
stylized %>% sable()
```


## Inference

Create robust CIs using the sandwich estimator.

```{r}
# Calculate robust standard errors
robust_se <- sandwich::vcovHC(fit, type = "HC0")

# Compute robust confidence intervals
robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)
```


Stylize results.

```{r}
# Summarize model coefficients
fit_summary <-
  robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
multivariable_pvalue <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
multivariable_pvalue %>% sable()
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=6}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(predictors) > 0) {
  
# Create table
coef_table <-
  robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(estimate)) %>%
  mutate(
    term = prettify_terms(term, cols),
    term = factor(term, levels = term)
  ) %>%
  # Cup the values to make the figure readable
  mutate(
    conf.low = ifelse(conf.high > 50, 50, conf.high),
    conf.high = ifelse(conf.high > 50, 50, conf.high),
    estimate = ifelse(estimate > 50, 50, estimate)
  )

# Plot
coef_table %>%
  ggplot(aes(x = estimate, y = term, color = term)) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
  labs(x = "Odds ratio", y = NULL) +
  guides(color = F) +
  scale_x_continuous(
    breaks = as.integer(seq(0, max(coef_table$conf.high), length.out = 6))
  ) +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )
}
```


## Predict

Predict.

```{r}
# Predicted probabilities
predicted_probs <- predict(fit, type = "response")
```


Plot histogram of predictions.

```{r, fig.width=6, fig.height=6}
tibble(
  Predictions = predicted_probs,
  Truth = fit$model[, CHOSEN_OUTCOME]
) %>%
  pivot_longer(
    cols = everything(),
    names_to = "keys",
    values_to = "values"
  ) %>%
  mutate(keys = fct_inorder(keys)) %>%
  ggplot(aes(x = values, color = keys, fill = keys)) +
  geom_histogram(alpha = 0.7) +
  geom_vline(xintercept = 0.5, linetype = 2, alpha = 0.5) +
  facet_wrap(vars(keys), ncol = 1, scales = "fixed") +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text = element_text(size = 12),
    strip.text.x = element_text(size = 13),
    legend.position = "none"
  )
```


## Evaluate

How many examples were used?

```{r}
# All examples
num_all_examples <- nrow(fit$data)
num_all_examples_positive <- sum(fit$data[, CHOSEN_OUTCOME], na.rm = T)
num_complete_cases <- nrow(fit$model)
num_complete_cases_positive <- sum(fit$model[, CHOSEN_OUTCOME], na.rm = T)

# Print
sprintf(
  "
  All examples = %s (%s with outcome)
  Fitted examples = %s (%s with outcome)",
  num_all_examples,
  num_all_examples_positive,
  num_complete_cases,
  num_complete_cases_positive
) %>% cat()
```


Residual analysis.

```{r}
# Residuals
residuals <- residuals(fit, type = "deviance")

# Plot residuals
plot(residuals)
```


Pseudo R-squared.

```{r}
# Calculate Pseudo R-squared values
pseudo_r2 <- pscl::pR2(fit)
print(pseudo_r2)
```


ROC-AUC.

```{r}
# Compute
auroc <- cvAUC::ci.cvAUC(
  predictions = predicted_probs,
  labels = fit$model[, CHOSEN_OUTCOME]
)

# Print
sprintf(
  "Cross-validated AUROC = %.3f (SE, %.3f; %s%% CI, %.3f-%.3f)",
  auroc$cvAUC,
  auroc$se,
  auroc$confidence * 100,
  auroc$ci[1],
  auroc$ci[2]
)
```


PR-AUC.

```{r}
# Construct object
precrec_obj_tr <- precrec::evalmod(
  scores = predicted_probs,
  labels = fit$model[, CHOSEN_OUTCOME],
  calc_avg = F,
  cb_alpha = 0.05
)

# Print
# NOTE: Can use precrec::auc_ci to get CIs, but need to have multiple datasets
precrec_obj_tr %>%
  precrec::part() %>%
  precrec::pauc() %>%
  sable()
```


ROC and PR curves.

<!-- Note that you can specify multiple models to get confidence intervals around the ROC: https://cran.r-project.org/web/packages/precrec/vignettes/introduction.html#:\~:text=evalmod%20and%20mmdata%20with%20cross%20validation%20datasets (without specifying multiple models the `show_cb = T` approach will give an error). -->

```{r, fig.height=3, fig.width=6}
# Plot
precrec_obj_tr %>% autoplot()
```


Another version of the ROC curve.

```{r}
# Calculate ROC curve
roc_curve <- pROC::roc(fit$model[, CHOSEN_OUTCOME] ~ predicted_probs)

# Plot ROC curve
plot(roc_curve)

# Calculate AUC
auc <- pROC::auc(roc_curve)
print(auc)
```


Plot all performance measures.

```{r, fig.height=6, fig.width=6}
# Construct object
precrec_obj_tr2 <- precrec::evalmod(
  scores = predicted_probs,
  labels = fit$model[, CHOSEN_OUTCOME],
  mode = "basic"
)

# Plot
precrec_obj_tr2 %>% autoplot()
```


**Confusion matrix.**

**Kappa statistic:** 0 = No agreement; 0.1-0.2 = Slight agreement; 0.2-0.4 = Fair agreement; 0.4-0.6 = Moderate agreement; 0.6-0.8 = Substantial agreement; 0.8 - 1 = Near perfect agreement

**No Information Rate (NIR):** The proportion of the largest observed class - for example, if we had 35 patients and 23 patients had the outcome, this is the largest class and the NIR is 23/35 = 0.657. The larger the deviation of Accuracy from NIR the more confident we are that the model is not just choosing the largest class.

```{r}
# Scores
pred <- factor(ifelse(predicted_probs > 0.5, "Event", "No Event"))
truth <- factor(ifelse(fit$model[, CHOSEN_OUTCOME], "Event", "No Event"))

# Count values per category
xtab <- table(
  scores = pred,
  labels = truth
)

# Compute confusion matrix - only if the model predicts both events
if (nlevels(pred) > 1) {
  confusion_matrix <-
    caret::confusionMatrix(
      xtab,
      positive = "Event",
      prevalence = NULL, # Provide prevalence as a proportion here if you want
      mode = "sens_spec"
  )

  # Print
  confusion_matrix
}
```


## Variable importance

Plot importance (based on magnitude of z-score).

```{r, fig.height=4, fig.width=6}
# Calculate importance
varimp <-
  fit %>%
  caret::varImp() %>%
  as_tibble(rownames = "rn") %>%
  mutate(Predictor = factor(rn) %>% fct_reorder(Overall)) %>%
  dplyr::select(Predictor, Importance = Overall)

# Plot variable importance
varimp %>%
  ggplot(aes(Predictor, Importance, fill = Importance)) +
  geom_bar(stat = "identity", alpha = 0.9, width = 0.65) +
  coord_flip() +
  guides(fill = F) +
  labs(y = "\nVariable Importance", x = NULL) +
  scale_fill_viridis_c() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.border = element_blank(),
    axis.text.x = element_text(margin = margin(t = 7), size = 12),
    axis.text.y = element_text(margin = margin(r = -10), size = 12),
    axis.title = element_text(size = 13)
  )
```


------------------------------------------------------------------------

# Multivariable - Without components

## Feature selection

Create predictors.

```{r}
# Define p-value threshold
pval_threshold <- 0.2

# Get the predictors of interest
predictors <-
  univariable_unadjusted_recoded %>%
  bind_rows(univariable_adjusted) %>%
  filter(`P-values` < pval_threshold) %>%
  pull(`Predictors`)

# Clean categorical variables
predictors <-
  predictors %>%
  str_replace("^([a-z0-9_]*)([A-Z].*)$", "\\1") %>%
  unique()

# Add adjustment variables
predictors <- unique(c(predictors))

# Remove unwanted predictors
predictors <- predictors[!predictors %in% unwanted_with_gradings]

# Print
print(predictors)
```


## Model selection

Fit logistic.

```{r}
# Define data
df <- df_multi

# Create formula
model <- as.formula(paste(
  CHOSEN_OUTCOME,
  "~",
  paste(predictors, collapse = " + ")
))

fit <- glm(
  model,
  data = df,
  family = binomial()
)

fit_with_grading <- fit
```


Print results.

```{r}
# Summarize model coefficients
fit_summary <-
  fit %>%
  broom::tidy(exponentiate = T, conf.int = T) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
stylized <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
stylized %>% sable()
```


## Inference

Create robust CIs using the sandwich estimator.

```{r}
# Calculate robust standard errors
robust_se <- sandwich::vcovHC(fit, type = "HC0")

# Compute robust confidence intervals
robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)
```


Stylize results.

```{r}
# Summarize model coefficients
fit_summary <-
  robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
multivariable_pvalue <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
multivariable_pvalue %>% sable()
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=6}
# Collect all columns
cols <- c(
  EXPOSURES_CONTINUOUS,
  EXPOSURES_BINARY,
  EXPOSURES_CATEGORICAL
)

# Define function to replace ugly with pretty names
prettify_terms <- function(old_terms, col_names = cols) {
  
  # Extract factor terms
  factor_terms <- str_extract(old_terms, "[A-Z].*")
  
  # Extract new terms
  base_terms <- str_replace(old_terms, "[A-Z].*$", "")
  new_terms <- map_chr(base_terms, ~ names(col_names[col_names == .x]))
  
  # Add the TRUE label
  new_terms <- ifelse(
    is.na(factor_terms),
    new_terms,
    paste(new_terms, factor_terms, sep = " - ")
  )
  
  return(new_terms)
}

if (length(predictors) > 0) {
  
# Create table
coef_table <-
  robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(estimate)) %>%
  mutate(
    term = prettify_terms(term, cols),
    term = factor(term, levels = term)
  ) %>%
  # Cup the values to make the figure readable
  mutate(
    conf.low = ifelse(conf.high > 50, 50, conf.high),
    conf.high = ifelse(conf.high > 50, 50, conf.high),
    estimate = ifelse(estimate > 50, 50, estimate)
  )

# Plot
coef_table %>%
  ggplot(aes(x = estimate, y = term, color = term)) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
  labs(x = "Odds ratio", y = NULL) +
  guides(color = F) +
  scale_x_continuous(
    breaks = as.integer(seq(0, max(coef_table$conf.high), length.out = 6))
  ) +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )
}
```


## Predict

Predict.

```{r}
# Predicted probabilities
predicted_probs <- predict(fit, type = "response")
```


Plot histogram of predictions.

```{r, fig.width=6, fig.height=6}
tibble(
  Predictions = predicted_probs,
  Truth = fit$model[, CHOSEN_OUTCOME]
) %>%
  pivot_longer(
    cols = everything(),
    names_to = "keys",
    values_to = "values"
  ) %>%
  mutate(keys = fct_inorder(keys)) %>%
  ggplot(aes(x = values, color = keys, fill = keys)) +
  geom_histogram(alpha = 0.7) +
  geom_vline(xintercept = 0.5, linetype = 2, alpha = 0.5) +
  facet_wrap(vars(keys), ncol = 1, scales = "fixed") +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text = element_text(size = 12),
    strip.text.x = element_text(size = 13),
    legend.position = "none"
  )
```


## Evaluate

How many examples were used?

```{r}
# All examples
num_all_examples <- nrow(fit$data)
num_all_examples_positive <- sum(fit$data[, CHOSEN_OUTCOME], na.rm = T)
num_complete_cases <- nrow(fit$model)
num_complete_cases_positive <- sum(fit$model[, CHOSEN_OUTCOME], na.rm = T)

# Print
sprintf(
  "
  All examples = %s (%s with outcome)
  Fitted examples = %s (%s with outcome)",
  num_all_examples,
  num_all_examples_positive,
  num_complete_cases,
  num_complete_cases_positive
) %>% cat()
```


Residual analysis.

```{r}
# Residuals
residuals <- residuals(fit, type = "deviance")

# Plot residuals
plot(residuals)
```


Pseudo R-squared.

```{r}
# Calculate Pseudo R-squared values
pseudo_r2 <- pscl::pR2(fit)
print(pseudo_r2)
```


ROC-AUC.

```{r}
# Compute
auroc <- cvAUC::ci.cvAUC(
  predictions = predicted_probs,
  labels = fit$model[, CHOSEN_OUTCOME]
)

# Print
sprintf(
  "Cross-validated AUROC = %.3f (SE, %.3f; %s%% CI, %.3f-%.3f)",
  auroc$cvAUC,
  auroc$se,
  auroc$confidence * 100,
  auroc$ci[1],
  auroc$ci[2]
)
```


PR-AUC.

```{r}
# Construct object
precrec_obj_tr <- precrec::evalmod(
  scores = predicted_probs,
  labels = fit$model[, CHOSEN_OUTCOME],
  calc_avg = F,
  cb_alpha = 0.05
)

# Print
# NOTE: Can use precrec::auc_ci to get CIs, but need to have multiple datasets
precrec_obj_tr %>%
  precrec::part() %>%
  precrec::pauc() %>%
  sable()
```


ROC and PR curves.

<!-- Note that you can specify multiple models to get confidence intervals around the ROC: https://cran.r-project.org/web/packages/precrec/vignettes/introduction.html#:\~:text=evalmod%20and%20mmdata%20with%20cross%20validation%20datasets (without specifying multiple models the `show_cb = T` approach will give an error). -->

```{r, fig.height=3, fig.width=6}
# Plot
precrec_obj_tr %>% autoplot()
```


Another version of the ROC curve.

```{r}
# Calculate ROC curve
roc_curve <- pROC::roc(fit$model[, CHOSEN_OUTCOME] ~ predicted_probs)

# Plot ROC curve
plot(roc_curve)

# Calculate AUC
auc <- pROC::auc(roc_curve)
print(auc)
```


Plot all performance measures.

```{r, fig.height=6, fig.width=6}
# Construct object
precrec_obj_tr2 <- precrec::evalmod(
  scores = predicted_probs,
  labels = fit$model[, CHOSEN_OUTCOME],
  mode = "basic"
)

# Plot
precrec_obj_tr2 %>% autoplot()
```


**Confusion matrix.**

**Kappa statistic:** 0 = No agreement; 0.1-0.2 = Slight agreement; 0.2-0.4 = Fair agreement; 0.4-0.6 = Moderate agreement; 0.6-0.8 = Substantial agreement; 0.8 - 1 = Near perfect agreement

**No Information Rate (NIR):** The proportion of the largest observed class - for example, if we had 35 patients and 23 patients had the outcome, this is the largest class and the NIR is 23/35 = 0.657. The larger the deviation of Accuracy from NIR the more confident we are that the model is not just choosing the largest class.

```{r}
# Scores
pred <- factor(ifelse(predicted_probs > 0.5, "Event", "No Event"))
truth <- factor(ifelse(fit$model[, CHOSEN_OUTCOME], "Event", "No Event"))

# Count values per category
xtab <- table(
  scores = pred,
  labels = truth
)

# Compute confusion matrix - only if the model predicts both events
if (nlevels(pred) > 1) {
  confusion_matrix <-
    caret::confusionMatrix(
      xtab,
      positive = "Event",
      prevalence = NULL, # Provide prevalence as a proportion here if you want
      mode = "sens_spec"
  )

  # Print
  confusion_matrix
}
```


## Variable importance

Plot importance (based on magnitude of z-score).

```{r, fig.height=4, fig.width=6}
# Calculate importance
varimp <-
  fit %>%
  caret::varImp() %>%
  as_tibble(rownames = "rn") %>%
  mutate(Predictor = factor(rn) %>% fct_reorder(Overall)) %>%
  dplyr::select(Predictor, Importance = Overall)

# Plot variable importance
varimp %>%
  ggplot(aes(Predictor, Importance, fill = Importance)) +
  geom_bar(stat = "identity", alpha = 0.9, width = 0.65) +
  coord_flip() +
  guides(fill = F) +
  labs(y = "\nVariable Importance", x = NULL) +
  scale_fill_viridis_c() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.border = element_blank(),
    axis.text.x = element_text(margin = margin(t = 7), size = 12),
    axis.text.y = element_text(margin = margin(r = -10), size = 12),
    axis.title = element_text(size = 13)
  )
```

------------------------------------------------------------------------


# Multivariable - With interactions

## Presents with hemorrhage

Fit logistic.

```{r}
# Define data
df <- df_multi

# Create formula
model <- as.formula(paste(
  CHOSEN_OUTCOME,
  "~",
  "modified_rankin_score_pretreatment*has_hemorrhage +
  spetzler_martin_grade*has_hemorrhage +
  age_at_first_treatment_yrs*has_hemorrhage"
))

fit <- glm(
  model,
  data = df,
  family = binomial()
)
```


Print results.

```{r}
# Summarize model coefficients
fit_summary <-
  fit %>%
  broom::tidy(exponentiate = T, conf.int = T) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
stylized <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
stylized %>% sable()
```

------------------------------------------------------------------------


# Model comparison

## Likelihood ratio

No statistical evidence that the grading-based model fits the data better than the grading-independent model based on the likelihood ratio test.

```{r}
anova(fit_without_grading, fit_with_grading, test = "LR")
```


------------------------------------------------------------------------

# Special cases

## SM < 4

Select features.

```{r}
# Define p-value threshold
pval_threshold <- 0.2

# Get the predictors of interest
predictors <-
  univariable_adjusted_recoded %>%
  bind_rows(univariable_unadjusted_recoded) %>%
  filter(`P-values` < pval_threshold) %>%
  pull(`Predictors`)

# Clean categorical variables
predictors <-
  predictors %>%
  str_replace("^([a-z0-9_]*)([A-Z].*)$", "\\1") %>%
  unique()

# Add adjustment variables
predictors <- unique(c(predictors))

# Remove unwanted predictors
predictors <- predictors[!predictors %in% unwanted_with_gradings]

# Replace SM by its binary variant
predictors <- c(predictors, "is_spetzler_martin_grade_less_than_4")
predictors <- predictors[!predictors == "spetzler_martin_grade"]

# Print
print(predictors)
```


Fit logistic.

```{r}
# Define data
df <- df_multi %>% drop_na(modified_rankin_score_pretreatment, location)

# Create formula
model <- as.formula(paste(
  CHOSEN_OUTCOME,
  "~",
  paste(predictors, collapse = " + ")
))

fit <- glm(
  model,
  data = df,
  family = binomial()
)

# Save
fit_without_grading <- fit
```


Print results.

```{r}
# Summarize model coefficients
fit_summary <-
  fit %>%
  broom::tidy(exponentiate = T, conf.int = T) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
stylized <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
stylized %>% sable()
```


Create robust CIs using the sandwich estimator.

```{r}
# Calculate robust standard errors
robust_se <- sandwich::vcovHC(fit, type = "HC0")

# Compute robust confidence intervals
robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)
```


Stylize results.

```{r}
# Summarize model coefficients
fit_summary <-
  robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
multivariable_pvalue <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
multivariable_pvalue %>% sable()
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(estimate)) %>%
  mutate(term = factor(term, levels = term)) %>%
  ggplot(aes(x = estimate, y = term, color = term)) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
  labs(x = "Odds ratio", y = NULL) +
  guides(color = F) +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )

# Dotwhisker was not giving me the correct plot
# dotwhisker::dwplot(
#   vline = geom_vline(xintercept = 1, linetype = 2, alpha =  0.5),
#   show_intercept = F
# )
```


## Had surgery

Select features.

```{r}
# Define p-value threshold
pval_threshold <- 0.2

# Get the predictors of interest
predictors <-
  univariable_adjusted_recoded %>%
  bind_rows(univariable_unadjusted_recoded) %>%
  filter(`P-values` < pval_threshold) %>%
  pull(`Predictors`)

# Clean categorical variables
predictors <-
  predictors %>%
  str_replace("^([a-z0-9_]*)([A-Z].*)$", "\\1") %>%
  unique()

# Add adjustment variables
predictors <- unique(c(predictors))

# Remove unwanted predictors
predictors <- predictors[!predictors %in% unwanted_with_gradings]

# Replace SM by its binary variant
predictors <- c(predictors, "is_surgery")
predictors <- predictors[!predictors == "procedure_combinations"]

# Print
print(predictors)
```


Fit logistic.

```{r}
# Define data
df <- df_multi %>% drop_na(modified_rankin_score_pretreatment, location)

# Create formula
model <- as.formula(paste(
  CHOSEN_OUTCOME,
  "~",
  paste(predictors, collapse = " + ")
))

fit <- glm(
  model,
  data = df,
  family = binomial()
)

# Save
fit_without_grading <- fit
```


Print results.

```{r}
# Summarize model coefficients
fit_summary <-
  fit %>%
  broom::tidy(exponentiate = T, conf.int = T) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
stylized <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
stylized %>% sable()
```


Create robust CIs using the sandwich estimator.

```{r}
# Calculate robust standard errors
robust_se <- sandwich::vcovHC(fit, type = "HC0")

# Compute robust confidence intervals
robust_ci <- lmtest::coeftest(fit, vcov. = robust_se)
```


Stylize results.

```{r}
# Summarize model coefficients
fit_summary <-
  robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  arrange(term == "(Intercept)", p.value) %>%
  rename(odds_ratio = estimate, z_value = statistic)

# Stylize and print
multivariable_pvalue <-
  fit_summary %>%
  rename(
    "Predictors" = term,
    "Odds Ratios (OR)" = odds_ratio,
    "SE" = std.error,
    "Z-scores" = z_value,
    "P-values" = p.value,
    "CI (low)" = conf.low,
    "CI (high)" = conf.high,
  ) %>%
  mutate(
    `Odds Ratios (OR)` = round(`Odds Ratios (OR)`, 2),
    `SE` = round(`SE`, 2),
    `CI (low)` = round(`CI (low)`, 2),
    `CI (high)` = round(`CI (high)`, 2),
    `P-values` = round(`P-values`, 3),
  )

# Print
multivariable_pvalue %>% sable()
```


Plot the coefficients with their confidence intervals.

```{r, fig.height=4, fig.width=8}
robust_ci %>%
  # broom does not support exponentiation after coeftest so do it manually
  broom::tidy(exponentiate = F, conf.int = T) %>%
  mutate(across(c(estimate, "conf.low", "conf.high"), exp)) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(estimate)) %>%
  mutate(term = factor(term, levels = term)) %>%
  ggplot(aes(x = estimate, y = term, color = term)) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 1, linetype = 2, alpha = 0.5) +
  labs(x = "Odds ratio", y = NULL) +
  guides(color = F) +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )

# Dotwhisker was not giving me the correct plot
# dotwhisker::dwplot(
#   vline = geom_vline(xintercept = 1, linetype = 2, alpha =  0.5),
#   show_intercept = F
# )
```


## Grade 5s

```{r}
df_uni %>%
  drop_na(spetzler_martin_grade) %>%
  count(is_poor_outcome, spetzler_martin_grade, name = "num_patients") %>%
  mutate(
    pct_patients = scales::percent(num_patients / sum(num_patients), 1)
  ) %>%
  sable()
```


Only consider those with SM grade 5.

```{r}
df_uni %>%
  filter(spetzler_martin_grade == 5) %>%
  count(is_poor_outcome, spetzler_martin_grade, name = "num_patients") %>%
  mutate(
    pct_patients = scales::percent(num_patients / sum(num_patients), 1)
  ) %>%
  sable()
```


------------------------------------------------------------------------

# Honest esitmation (WIP)

https://www.pnas.org/doi/full/10.1073/pnas.1510489113

https://arxiv.org/pdf/1510.04342

https://github.com/grf-labs/grf

https://grf-labs.github.io/grf/REFERENCE.html

https://cran.r-project.org/web/packages/hettx/vignettes/detect_idiosyncratic_vignette.html


```{r eval=FALSE, include=FALSE}
library(grf)
```


```{r eval=FALSE, include=FALSE}
# Generate data.
n <- 2000
p <- 10
X <- matrix(rnorm(n * p), n, p)
X.test <- matrix(0, 101, p)
X.test[, 1] <- seq(-2, 2, length.out = 101)

# Train a causal forest.
W <- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] > 0))
Y <- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
tau.forest <- causal_forest(X, Y, W)

# Estimate treatment effects for the training data using out-of-bag prediction.
tau.hat.oob <- predict(tau.forest)
hist(tau.hat.oob$predictions)

# Estimate treatment effects for the test sample.
tau.hat <- predict(tau.forest, X.test)
plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2)

# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(tau.forest, target.sample = "all")

# Estimate the conditional average treatment effect on the treated sample (CATT).
average_treatment_effect(tau.forest, target.sample = "treated")

# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.forest <- causal_forest(X, Y, W, num.trees = 4000)
tau.hat <- predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat <- sqrt(tau.hat$variance.estimates)
plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[, 1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[, 1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 1)
```


------------------------------------------------------------------------

# Write

## Setup

Create the necessary directories.

```{r, results="hold"}
# Get today's date
today <- Sys.Date()
today <- format(today, "%Y-%m-%d")

# Create folder names
base_folder <- file.path(DST_DIRNAME, ANALYSIS_NAME)
date_folder <- file.path(base_folder, today)
csv_folder <- file.path(date_folder, "csv")
pdf_folder <- file.path(date_folder, "pdf")
html_folder <- file.path(date_folder, "html")

# Create folders
suppressWarnings(dir.create(base_folder))
suppressWarnings(dir.create(date_folder))
suppressWarnings(dir.create(csv_folder))
suppressWarnings(dir.create(pdf_folder))
suppressWarnings(dir.create(html_folder))

# Print folder names
print(base_folder)
print(date_folder)
print(csv_folder)
print(pdf_folder)
print(html_folder)
```


## Figures

Write all figures.

```{r}
# Save
ggsave(
  file.path(pdf_folder, "sm_grade_vs_complications_bar_chart.pdf"),
  plot = plots$sm_grade_vs_complications_bar_chart,
  width = 8,
  height = 6
)
```

```{r}
# Save
ggsave(
  file.path(pdf_folder, "odds_ratios_all_predictors.pdf"),
  plot = plots$odds_ratios_all_predictors,
  width = 8,
  height = 6
)
```

```{r}
# Save
ggsave(
  file.path(pdf_folder, "odds_ratios_with_components.pdf"),
  plot = plots$odds_ratios_with_components,
  width = 8,
  height = 6
)
```

```{r}
# Save
ggsave(
  file.path(pdf_folder, "odds_ratios_without_components.pdf"),
  plot = plots$odds_ratios_without_components,
  width = 8,
  height = 6
)
```

```{r}
# Save
if (with(plots, exists("odds_ratios_with_components_no_surgery"))) {
  ggsave(
    file.path(pdf_folder, "odds_ratios_with_components_no_surgery.pdf"),
    plot = plots$odds_ratios_with_components_no_surgery,
    width = 8,
    height = 6
  )
}
```

```{r}
# Save
if (with(plots, exists("odds_ratios_without_components_no_surgery"))) {
  ggsave(
    file.path(pdf_folder, "odds_ratios_without_components_no_surgery.pdf"),
    plot = plots$odds_ratios_without_components_no_surgery,
    width = 8,
    height = 6
  )
}
```

```{r}
# # Start graphic device
# pdf(
#   file = file.path(pdf_folder, "forest-plot_frequentist.pdf"),
#   width = 10,
#   height = 15
# )
#
# # Plot
# plots$forest_plot_frequentist
#
# # Shut down device
# dev.off()
```


## Tables

Write all tables.

```{r}
# # Arguments
# df <- tables$desc_stats_cohorts_cv_prolaio
# filepath_csv <- file.path(csv_folder, "desc-stats_by-cohort_cov.csv")
# filepath_html <- file.path(html_folder, "desc-stats_by-cohort_cov.html")
#
# # Save as CSV
# write_csv(
#   x = df,
#   file = filepath_csv
# )
#
# # Save as HTML
# # - Save pdf does not work with webshot
# # - It works with pagedown but not as pretty as desired
# df %>%
#   sable() %>%
#   kableExtra::save_kable(file = filepath_html)
```

```{r}
write_csv(
  univariable_unadjusted,
  file.path(csv_folder, "univariable_unadjusted.csv")
)
```

```{r}
write_csv(
  univariable_adjusted,
  file.path(csv_folder, "univariable_adjusted.csv")
)
```

```{r}
write_csv(
  multivariable_pvalue,
  file.path(csv_folder, "multivariable_pvalue.csv")
)
```


## Data

Write all data.

```{r}
# # Arguments
# df <- study
# filename <- paste0(ANALYSIS_NAME, "_", today, ".csv")
# filepath_csv <- file.path(DST_BUCKET, dst_dirname_data, filename)
#
# # Save as CSV
# write_csv(
#   x = df,
#   file = filepath_csv
# )
```


------------------------------------------------------------------------

# Reproducibility

## Linting and styling

```{r, eval=FALSE}
# Style current file
styler::style_file(
  path = rstudioapi::getSourceEditorContext()$path,
  style = styler::tidyverse_style
)

# Lint current file
lintr::lint(rstudioapi::getSourceEditorContext()$path)
```


## Dependency management

```{r, eval=FALSE, results="hide"}
# Clean up project of libraries not in use
# (use prompt = FALSE to avoid the interactive session)
# (packages can only be removed in interactive mode b/c this is destructive)
renv::clean(prompt = TRUE)

# Update lock file with new packages
renv::snapshot()
```


## Containerization

```{r}
# Only run this if option is set to TRUE
if (UPDATE_DOCKERFILE) {
  # Create a dockerfile from the session info
  my_dockerfile <- containerit::dockerfile(from = sessionInfo(), env = ls())
  # Write file
  write(my_dockerfile, file = "~/Dockerfile")
  # Print
  print(my_dockerfile)
}
```


------------------------------------------------------------------------

# Documentation {.tabset}

## Session Info

```{r session_info, echo=FALSE}
print(sessionInfo(), locale = FALSE)
```

## References

```{r refs, echo=FALSE}
(.packages()) %>%
  sort() %>%
  lapply(citation) %>%
  lapply(c) %>%
  unique()
```
